{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7686600,"sourceType":"datasetVersion","datasetId":4485408},{"sourceId":7929032,"sourceType":"datasetVersion","datasetId":4660271},{"sourceId":7933765,"sourceType":"datasetVersion","datasetId":4663623}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport time\n\n# text NLP\nimport nltk\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport string\nfrom wordcloud import WordCloud\nfrom nltk.corpus import words\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom unidecode import unidecode\n\n# Preprocessing\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\n\n# model\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n# Score\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import KFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-25T01:55:04.598618Z","iopub.execute_input":"2024-03-25T01:55:04.598975Z","iopub.status.idle":"2024-03-25T01:55:08.498819Z","shell.execute_reply.started":"2024-03-25T01:55:04.598947Z","shell.execute_reply":"2024-03-25T01:55:08.497700Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning & Preprocessing","metadata":{}},{"cell_type":"code","source":"# import nltk\n# import subprocess\n\n# # Download and unzip wordnet\n# try:\n#     nltk.data.find('wordnet.zip')\n# except:\n#     nltk.download('wordnet', download_dir='/kaggle/working/')\n#     command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n#     subprocess.run(command.split())\n#     nltk.data.path.append('/kaggle/working/')\n\n# # Now you can import the NLTK resources as usual\n# from nltk.corpus import wordnet","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:43:54.816250Z","iopub.execute_input":"2024-03-24T09:43:54.817111Z","iopub.status.idle":"2024-03-24T09:43:54.821379Z","shell.execute_reply.started":"2024-03-24T09:43:54.817077Z","shell.execute_reply":"2024-03-24T09:43:54.820334Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# class BasicTextCleaning:\n#     def __init__(self):\n#         # define some necessary elements\n#         self.stopwords = set(stopwords.words('english'))\n#         self.words_corpus = set(words.words())\n#         self.stemmer = PorterStemmer()\n#         self.lemmatizer = WordNetLemmatizer()\n\n#         # dictionary of methods can be used\n#         self.methods = {'lowercase': str.lower,\n#                         'accent_removal': self.accent_removal,\n#                         'strip': str.strip,\n#                         'nice_display': self.nice_display,\n#                         'tokenization': nltk.word_tokenize,\n#                         'stemming': self.stemming,\n#                         'lemmatization': self.lemmatization,\n#                         'punctuation_removal': self.punctuation_removal,\n#                         'stopwords_removal': self.stopwords_removal,\n#                         'contractions_expand': self.contractions_expand,\n#                         'nonsense_removal': self.nonsense_removal,\n#                         'number_removal': self.number_removal}\n\n#         self.punctuations = '[%s]' % re.escape(string.punctuation)\n\n#     def text_cleaning(self, texts, methods=None):\n#         if not methods:\n#             methods = ['accent_removal', 'lowercase', 'nice_display', 'punctuation_removal',\n#                        'stopwords_removal', 'lemmatization', 'stemming']\n#         if isinstance(texts, str):\n#             texts = [texts]\n#         cleaned_texts = []\n#         for text in texts:\n#             for method in methods:\n#                 if method not in self.methods.keys():\n#                     raise Warning('Invalid method \"{}\". Basic text cleaning methods available: {}'.format(method, \", \".join(self.methods.keys())))\n#                 text = self.methods[method](text)\n#             cleaned_texts.append(text)\n#         return cleaned_texts\n\n#     def strip_text(self, text):\n#         return text.strip()\n\n#     def lowercase(self, text):\n#         return text.lower()\n\n#     def contractions_expand(self, text):\n#         return contractions.fix(text)\n\n#     def number_removal(self, text):\n#         text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n#         text = re.sub(r'\\s+', ' ', text)\n#         return text\n\n#     def nice_display(self, text):\n#         text = re.sub(r\"([^\\w\\s([{\\'])(\\w)\", r\"\\1 \\2\", text)\n#         text = re.sub(r'\\s+', ' ', text)\n#         return text.strip()\n\n#     def accent_removal(self, text):\n#         text = unidecode(text)\n#         return text\n\n#     def punctuation_removal(self, text):\n#         text = re.sub(self.punctuations, ' ', text)\n#         text = re.sub(r'\\s+', ' ', text)\n#         return text.strip()\n\n#     def stopwords_removal(self, text):\n#         return \" \".join([word for word in text.split() if word not in self.stopwords])\n\n#     def stemming(self, text):\n#         return \" \".join([self.stemmer.stem(word) for word in text.split()])\n\n#     def lemmatization(self, text):\n#         return \" \".join([self.lemmatizer.lemmatize(word) for word in text.split()])\n\n#     def tokenization(self, text):\n#         return nltk.word_tokenize(text)\n\n#     def nonsense_removal(self, text):\n#         return \" \".join([word for word in text.split() if wordnet.synsets(word)])","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:44:09.192104Z","iopub.execute_input":"2024-03-24T09:44:09.192486Z","iopub.status.idle":"2024-03-24T09:44:09.199659Z","shell.execute_reply.started":"2024-03-24T09:44:09.192426Z","shell.execute_reply":"2024-03-24T09:44:09.198722Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# cleaning = BasicTextCleaning()\n# data['text'] = cleaning.text_cleaning(data['text_'])\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:44:20.842649Z","iopub.execute_input":"2024-03-24T09:44:20.843035Z","iopub.status.idle":"2024-03-24T09:44:20.847196Z","shell.execute_reply.started":"2024-03-24T09:44:20.843005Z","shell.execute_reply":"2024-03-24T09:44:20.846278Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/datacleaning/datacleaning.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:34:13.458814Z","iopub.execute_input":"2024-03-24T15:34:13.459643Z","iopub.status.idle":"2024-03-24T15:34:13.720259Z","shell.execute_reply.started":"2024-03-24T15:34:13.459613Z","shell.execute_reply":"2024-03-24T15:34:13.719418Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"data['label'].replace('CG', 1, inplace=True)\ndata['label'].replace('OR', 0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:34:16.019318Z","iopub.execute_input":"2024-03-24T15:34:16.019675Z","iopub.status.idle":"2024-03-24T15:34:16.059184Z","shell.execute_reply.started":"2024-03-24T15:34:16.019648Z","shell.execute_reply":"2024-03-24T15:34:16.058228Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/384321868.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data['label'].replace('CG', 1, inplace=True)\n/tmp/ipykernel_34/384321868.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data['label'].replace('OR', 0, inplace=True)\n/tmp/ipykernel_34/384321868.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  data['label'].replace('OR', 0, inplace=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"data = data.fillna('')","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:34:18.947234Z","iopub.execute_input":"2024-03-24T15:34:18.947604Z","iopub.status.idle":"2024-03-24T15:34:18.973435Z","shell.execute_reply.started":"2024-03-24T15:34:18.947574Z","shell.execute_reply":"2024-03-24T15:34:18.972450Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Save result","metadata":{}},{"cell_type":"code","source":"# data_save = pd.DataFrame(columns=['data','length_used', 'feature_extraction', 'feature_selection', 'model', 'accuracy', 'f1', 'recall', 'precision', 'roc_auc', 'notes'])\n# data_save.to_csv('results.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T14:21:21.632478Z","iopub.execute_input":"2024-03-22T14:21:21.633096Z","iopub.status.idle":"2024-03-22T14:21:21.642507Z","shell.execute_reply.started":"2024-03-22T14:21:21.633063Z","shell.execute_reply":"2024-03-22T14:21:21.641537Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def save_and_print(data, length_used, feature_extraction, feature_selection, model, scores):\n    data_save = pd.read_csv('/kaggle/working/results.csv')\n    for feature_extraction in feature_extractions:\n        sc = str(feature_extraction)\n        new_row = {'data': data, 'length_used': length_used, \n               'feature_extraction': feature_extraction, \n               'feature_selection': feature_selection, \n               'model': model, 'accuracy': np.mean(scores[sc]['accuracy']),\n               'f1': np.mean(scores[sc]['f1']), 'recall': np.mean(scores[sc]['recall']), \n               'precision': np.mean(scores[sc]['precision']), 'roc_auc': np.mean(scores[sc]['roc_auc']), \n               'notes': notes}\n        data_save.loc[len(data_save)] = new_row\n        data_save.to_csv('/kaggle/working/results.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T16:16:31.032156Z","iopub.execute_input":"2024-03-24T16:16:31.032824Z","iopub.status.idle":"2024-03-24T16:16:31.039771Z","shell.execute_reply.started":"2024-03-24T16:16:31.032792Z","shell.execute_reply":"2024-03-24T16:16:31.038852Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Case 1","metadata":{}},{"cell_type":"code","source":"dataname = 'Fake Review Dataset'\nlengh_used = None\nfeature_selection = None\nnotes = None\n","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:34:22.158916Z","iopub.execute_input":"2024-03-24T15:34:22.159492Z","iopub.status.idle":"2024-03-24T15:34:22.163848Z","shell.execute_reply.started":"2024-03-24T15:34:22.159463Z","shell.execute_reply":"2024-03-24T15:34:22.162829Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"X, y = data['text'], data['label']","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:34:23.750884Z","iopub.execute_input":"2024-03-24T15:34:23.751267Z","iopub.status.idle":"2024-03-24T15:34:23.755934Z","shell.execute_reply.started":"2024-03-24T15:34:23.751239Z","shell.execute_reply":"2024-03-24T15:34:23.754838Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2024-03-24T11:07:48.854061Z","iopub.execute_input":"2024-03-24T11:07:48.854740Z","iopub.status.idle":"2024-03-24T11:07:53.104661Z","shell.execute_reply.started":"2024-03-24T11:07:48.854706Z","shell.execute_reply":"2024-03-24T11:07:53.103726Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n\nfolds = kf.split(X, y)\nscores = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_lg = lgb.LGBMClassifier(force_row_wise=True)\n        model_lg.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_lg.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_lg.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores:\n            scores[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores[key]['accuracy'].append(accuracy)\n        scores[key]['f1'].append(f1)\n        scores[key]['recall'].append(recall)\n        scores[key]['precision'].append(precision)\n        scores[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T11:09:23.918631Z","iopub.execute_input":"2024-03-24T11:09:23.919366Z","iopub.status.idle":"2024-03-24T11:11:10.022005Z","shell.execute_reply.started":"2024-03-24T11:09:23.919328Z","shell.execute_reply":"2024-03-24T11:11:10.021053Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 25499\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 158972\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 3270\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 25498\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 159236\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 3276\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 25498\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 157115\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 3251\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 12749\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 25499\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 158775\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 3276\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 25497\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 158805\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 3296\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n","output_type":"stream"}]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lg, scores)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T11:33:36.436013Z","iopub.execute_input":"2024-03-24T11:33:36.436394Z","iopub.status.idle":"2024-03-24T11:33:36.467783Z","shell.execute_reply.started":"2024-03-24T11:33:36.436362Z","shell.execute_reply":"2024-03-24T11:33:36.466902Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"# XG boost","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2024-03-24T10:52:56.774583Z","iopub.execute_input":"2024-03-24T10:52:56.774975Z","iopub.status.idle":"2024-03-24T10:52:56.779533Z","shell.execute_reply.started":"2024-03-24T10:52:56.774943Z","shell.execute_reply":"2024-03-24T10:52:56.778518Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n\nfolds = kf.split(X, y)\nscores_xgb = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_xgb = XGBClassifier()\n        model_xgb.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_xgb.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_xgb.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_xgb:\n            scores_xgb[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_xgb[key]['accuracy'].append(accuracy)\n        scores_xgb[key]['f1'].append(f1)\n        scores_xgb[key]['recall'].append(recall)\n        scores_xgb[key]['precision'].append(precision)\n        scores_xgb[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T11:14:48.664840Z","iopub.execute_input":"2024-03-24T11:14:48.665743Z","iopub.status.idle":"2024-03-24T11:24:53.058440Z","shell.execute_reply.started":"2024-03-24T11:14:48.665707Z","shell.execute_reply":"2024-03-24T11:24:53.057464Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_xgb, scores_xgb)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T11:34:28.498893Z","iopub.execute_input":"2024-03-24T11:34:28.499635Z","iopub.status.idle":"2024-03-24T11:34:28.539799Z","shell.execute_reply.started":"2024-03-24T11:34:28.499598Z","shell.execute_reply":"2024-03-24T11:34:28.538706Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"# Adaboost","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:29:45.509216Z","iopub.execute_input":"2024-03-24T15:29:45.509568Z","iopub.status.idle":"2024-03-24T15:29:45.513992Z","shell.execute_reply.started":"2024-03-24T15:29:45.509544Z","shell.execute_reply":"2024-03-24T15:29:45.512934Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n\nfolds = kf.split(X, y)\nscores_ada = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_ada = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)\n        model_ada.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_ada.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_ada.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_ada:\n            scores_ada[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_ada[key]['accuracy'].append(accuracy)\n        scores_ada[key]['f1'].append(f1)\n        scores_ada[key]['recall'].append(recall)\n        scores_ada[key]['precision'].append(precision)\n        scores_ada[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:34:37.946831Z","iopub.execute_input":"2024-03-24T15:34:37.947748Z","iopub.status.idle":"2024-03-24T15:44:48.086181Z","shell.execute_reply.started":"2024-03-24T15:34:37.947713Z","shell.execute_reply":"2024-03-24T15:44:48.084958Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_ada, scores_ada)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T16:17:28.642738Z","iopub.execute_input":"2024-03-24T16:17:28.643415Z","iopub.status.idle":"2024-03-24T16:17:28.672748Z","shell.execute_reply.started":"2024-03-24T16:17:28.643384Z","shell.execute_reply":"2024-03-24T16:17:28.672022Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# CATBOOST","metadata":{}},{"cell_type":"code","source":"import catboost as cb","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:51:25.156266Z","iopub.execute_input":"2024-03-24T15:51:25.157302Z","iopub.status.idle":"2024-03-24T15:51:25.419700Z","shell.execute_reply.started":"2024-03-24T15:51:25.157265Z","shell.execute_reply":"2024-03-24T15:51:25.418949Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"folds = kf.split(X, y)\nscores_cat = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_cat = cb.CatBoostClassifier(\n            random_state=42,\n            task_type='GPU',\n            border_count=2 * len(feature_extractions)\n        )\n\n        model_cat.fit(\n            X_train_hashed,\n            y_train,\n            verbose=False,\n            plot=False,\n            early_stopping_rounds=50,\n            use_best_model=True,\n            eval_set=(X_test_hashed, y_test)\n        )\n\n        y_test_pred = model_cat.predict(X_test_hashed)\n\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_cat.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_cat:\n            scores_cat[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_cat[key]['accuracy'].append(accuracy)\n        scores_cat[key]['f1'].append(f1)\n        scores_cat[key]['recall'].append(recall)\n        scores_cat[key]['precision'].append(precision)\n        scores_cat[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:54:59.959174Z","iopub.execute_input":"2024-03-24T15:54:59.959777Z","iopub.status.idle":"2024-03-24T16:14:34.833742Z","shell.execute_reply.started":"2024-03-24T15:54:59.959746Z","shell.execute_reply":"2024-03-24T16:14:34.832801Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_cat, scores_cat)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T16:16:38.146955Z","iopub.execute_input":"2024-03-24T16:16:38.147849Z","iopub.status.idle":"2024-03-24T16:16:38.176076Z","shell.execute_reply.started":"2024-03-24T16:16:38.147814Z","shell.execute_reply":"2024-03-24T16:16:38.175073Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n\nfolds = kf.split(X, y)\nscores_lr = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_lr = LogisticRegression(max_iter = 1000)\n        model_lr.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_lr.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_lr.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_lr:\n            scores_lr[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_lr[key]['accuracy'].append(accuracy)\n        scores_lr[key]['f1'].append(f1)\n        scores_lr[key]['recall'].append(recall)\n        scores_lr[key]['precision'].append(precision)\n        scores_lr[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T12:10:22.334468Z","iopub.execute_input":"2024-03-15T12:10:22.334843Z","iopub.status.idle":"2024-03-15T12:13:07.614695Z","shell.execute_reply.started":"2024-03-15T12:10:22.334814Z","shell.execute_reply":"2024-03-15T12:13:07.613872Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model, scores)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T12:14:08.887605Z","iopub.execute_input":"2024-03-15T12:14:08.887924Z","iopub.status.idle":"2024-03-15T12:14:08.916658Z","shell.execute_reply.started":"2024-03-15T12:14:08.887900Z","shell.execute_reply":"2024-03-15T12:14:08.915827Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n                     HashingVectorizer(n_features = 100),HashingVectorizer()]\nmetrics_param = ['cosine','minkowski']\nneighbors_param = [5, 10, 50, 100]\nfolds = kf.split(X, y)\nscores_knn = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n        for metric in metrics_param:\n            for neighbor in neighbors_param:\n                model_knn = KNeighborsClassifier(metric = metric, n_neighbors = neighbor)\n                model_knn.fit(X_train_hashed, y_train)\n\n                y_test_pred = model_knn.predict(X_test_hashed)\n                accuracy = accuracy_score(y_test, y_test_pred)\n                f1 = f1_score(y_test, y_test_pred, pos_label=1)\n                recall = recall_score(y_test, y_test_pred)\n                precision = precision_score(y_test, y_test_pred)\n                roc_auc = roc_auc_score(y_test, model_knn.predict_proba(X_test_hashed)[:, 1])\n\n                key = str(feature_extraction)\n                if key not in scores_knn:\n                    scores_knn[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n                scores_knn[key]['accuracy'].append(accuracy)\n                scores_knn[key]['f1'].append(f1)\n                scores_knn[key]['recall'].append(recall)\n                scores_knn[key]['precision'].append(precision)\n                scores_knn[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T12:14:33.554596Z","iopub.execute_input":"2024-03-15T12:14:33.554948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_knn, scores_knn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result","metadata":{}},{"cell_type":"code","source":"result = pd.read_csv('/kaggle/input/result-1/results_1.csv')\nresult = result.sort_values(by='accuracy', ascending=False)\nresult","metadata":{"execution":{"iopub.status.busy":"2024-03-25T01:58:36.688806Z","iopub.execute_input":"2024-03-25T01:58:36.689160Z","iopub.status.idle":"2024-03-25T01:58:36.734031Z","shell.execute_reply.started":"2024-03-25T01:58:36.689132Z","shell.execute_reply":"2024-03-25T01:58:36.733018Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                   data  length_used                 feature_extraction  \\\n3   Fake Review Dataset          NaN                HashingVectorizer()   \n30  Fake Review Dataset          NaN                HashingVectorizer()   \n34  Fake Review Dataset          NaN                HashingVectorizer()   \n22  Fake Review Dataset          NaN                HashingVectorizer()   \n26  Fake Review Dataset          NaN                HashingVectorizer()   \n33  Fake Review Dataset          NaN  HashingVectorizer(n_features=100)   \n29  Fake Review Dataset          NaN  HashingVectorizer(n_features=100)   \n25  Fake Review Dataset          NaN  HashingVectorizer(n_features=100)   \n21  Fake Review Dataset          NaN  HashingVectorizer(n_features=100)   \n38  Fake Review Dataset          NaN                HashingVectorizer()   \n28  Fake Review Dataset          NaN   HashingVectorizer(n_features=50)   \n32  Fake Review Dataset          NaN   HashingVectorizer(n_features=50)   \n24  Fake Review Dataset          NaN   HashingVectorizer(n_features=50)   \n20  Fake Review Dataset          NaN   HashingVectorizer(n_features=50)   \n37  Fake Review Dataset          NaN  HashingVectorizer(n_features=100)   \n36  Fake Review Dataset          NaN   HashingVectorizer(n_features=50)   \n2   Fake Review Dataset          NaN  HashingVectorizer(n_features=100)   \n23  Fake Review Dataset          NaN   HashingVectorizer(n_features=10)   \n1   Fake Review Dataset          NaN   HashingVectorizer(n_features=50)   \n9   Fake Review Dataset          NaN   HashingVectorizer(n_features=50)   \n12  Fake Review Dataset          NaN   HashingVectorizer(n_features=50)   \n13  Fake Review Dataset          NaN  HashingVectorizer(n_features=100)   \n27  Fake Review Dataset          NaN   HashingVectorizer(n_features=10)   \n31  Fake Review Dataset          NaN   HashingVectorizer(n_features=10)   \n16  Fake Review Dataset          NaN   HashingVectorizer(n_features=50)   \n19  Fake Review Dataset          NaN   HashingVectorizer(n_features=10)   \n5   Fake Review Dataset          NaN   HashingVectorizer(n_features=50)   \n10  Fake Review Dataset          NaN  HashingVectorizer(n_features=100)   \n14  Fake Review Dataset          NaN                HashingVectorizer()   \n17  Fake Review Dataset          NaN  HashingVectorizer(n_features=100)   \n7   Fake Review Dataset          NaN                HashingVectorizer()   \n35  Fake Review Dataset          NaN   HashingVectorizer(n_features=10)   \n15  Fake Review Dataset          NaN   HashingVectorizer(n_features=10)   \n6   Fake Review Dataset          NaN  HashingVectorizer(n_features=100)   \n0   Fake Review Dataset          NaN   HashingVectorizer(n_features=10)   \n4   Fake Review Dataset          NaN   HashingVectorizer(n_features=10)   \n8   Fake Review Dataset          NaN   HashingVectorizer(n_features=10)   \n11  Fake Review Dataset          NaN   HashingVectorizer(n_features=10)   \n18  Fake Review Dataset          NaN                HashingVectorizer()   \n\n    feature_selection                                              model  \\\n3                 NaN                  LogisticRegression(max_iter=1000)   \n30                NaN  <catboost.core.CatBoostClassifier object at 0x...   \n34                NaN  <catboost.core.CatBoostClassifier object at 0x...   \n22                NaN                LGBMClassifier(force_row_wise=True)   \n26                NaN  XGBClassifier(base_score=None, booster=None, c...   \n33                NaN  <catboost.core.CatBoostClassifier object at 0x...   \n29                NaN  <catboost.core.CatBoostClassifier object at 0x...   \n25                NaN  XGBClassifier(base_score=None, booster=None, c...   \n21                NaN                LGBMClassifier(force_row_wise=True)   \n38                NaN               AdaBoostClassifier(n_estimators=100)   \n28                NaN  <catboost.core.CatBoostClassifier object at 0x...   \n32                NaN  <catboost.core.CatBoostClassifier object at 0x...   \n24                NaN  XGBClassifier(base_score=None, booster=None, c...   \n20                NaN                LGBMClassifier(force_row_wise=True)   \n37                NaN               AdaBoostClassifier(n_estimators=100)   \n36                NaN               AdaBoostClassifier(n_estimators=100)   \n2                 NaN                  LogisticRegression(max_iter=1000)   \n23                NaN  XGBClassifier(base_score=None, booster=None, c...   \n1                 NaN                  LogisticRegression(max_iter=1000)   \n9                 NaN              KNeighborsClassifier(metric='cosine')   \n12                NaN  KNeighborsClassifier(metric='cosine', n_neighb...   \n13                NaN  KNeighborsClassifier(metric='cosine', n_neighb...   \n27                NaN  <catboost.core.CatBoostClassifier object at 0x...   \n31                NaN  <catboost.core.CatBoostClassifier object at 0x...   \n16                NaN  KNeighborsClassifier(metric='cosine', n_neighb...   \n19                NaN                LGBMClassifier(force_row_wise=True)   \n5                 NaN  KNeighborsClassifier(metric='euclidean', n_nei...   \n10                NaN              KNeighborsClassifier(metric='cosine')   \n14                NaN  KNeighborsClassifier(metric='cosine', n_neighb...   \n17                NaN  KNeighborsClassifier(metric='cosine', n_neighb...   \n7                 NaN  KNeighborsClassifier(metric='euclidean', n_nei...   \n35                NaN               AdaBoostClassifier(n_estimators=100)   \n15                NaN  KNeighborsClassifier(metric='cosine', n_neighb...   \n6                 NaN  KNeighborsClassifier(metric='euclidean', n_nei...   \n0                 NaN                  LogisticRegression(max_iter=1000)   \n4                 NaN  KNeighborsClassifier(metric='euclidean', n_nei...   \n8                 NaN              KNeighborsClassifier(metric='cosine')   \n11                NaN  KNeighborsClassifier(metric='cosine', n_neighb...   \n18                NaN  KNeighborsClassifier(metric='cosine', n_neighb...   \n\n    accuracy        f1    recall  precision   roc_auc  notes  \n3   0.854472  0.854731  0.856315   0.853172  0.932805    NaN  \n30  0.849030  0.848443  0.845242   0.851696  0.931995    NaN  \n34  0.849030  0.848443  0.845242   0.851696  0.931995    NaN  \n22  0.844801  0.843937  0.839496   0.848431  0.930118    NaN  \n26  0.839112  0.836559  0.823757   0.849778  0.926102    NaN  \n33  0.803868  0.808183  0.826445   0.790737  0.891575    NaN  \n29  0.803868  0.808183  0.826445   0.790737  0.891575    NaN  \n25  0.799391  0.803480  0.820276   0.787402  0.888400    NaN  \n21  0.795978  0.796427  0.798369   0.794521  0.882648    NaN  \n38  0.779358  0.779414  0.779717   0.779169  0.870845    NaN  \n28  0.765483  0.770820  0.788956   0.753517  0.851934    NaN  \n32  0.765483  0.770820  0.788956   0.753517  0.851934    NaN  \n24  0.762317  0.767627  0.785162   0.750896  0.850886    NaN  \n20  0.752448  0.754085  0.759122   0.749139  0.837559    NaN  \n37  0.749530  0.757824  0.783928   0.733445  0.832393    NaN  \n36  0.717451  0.724684  0.743805   0.706553  0.793530    NaN  \n2   0.676049  0.678463  0.683620   0.673423  0.738315    NaN  \n23  0.634918  0.642519  0.656301   0.629349  0.688036    NaN  \n1   0.631950  0.635951  0.643011   0.629166  0.681856    NaN  \n9   0.630023  0.694295  0.846551   0.588672  0.703485    NaN  \n12  0.627733  0.693486  0.842666   0.589465  0.695421    NaN  \n13  0.624011  0.705586  0.901408   0.579871  0.712325    NaN  \n27  0.623936  0.633346  0.649673   0.617967  0.675575    NaN  \n31  0.623936  0.633346  0.649673   0.617967  0.675575    NaN  \n16  0.622378  0.706701  0.909949   0.577719  0.758787    NaN  \n19  0.619806  0.626396  0.637486   0.615838  0.670927    NaN  \n5   0.618647  0.689355  0.852387   0.579229  0.693508    NaN  \n10  0.615247  0.695359  0.884462   0.572979  0.663069    NaN  \n14  0.605881  0.707367  0.952705   0.562726  0.717157    NaN  \n17  0.605176  0.706435  0.950214   0.562239  0.777750    NaN  \n7   0.604674  0.705149  0.951818   0.560207  0.716450    NaN  \n35  0.604101  0.611954  0.624399   0.600114  0.646348    NaN  \n15  0.590794  0.606033  0.629740   0.584293  0.632423    NaN  \n6   0.580500  0.684094  0.913378   0.547310  0.676480    NaN  \n0   0.567818  0.580897  0.599094   0.563848  0.596594    NaN  \n4   0.560846  0.568853  0.584163   0.555935  0.579758    NaN  \n8   0.560096  0.564041  0.573705   0.556180  0.578870    NaN  \n11  0.556966  0.561606  0.568422   0.556642  0.574380    NaN  \n18  0.549874  0.685645  0.981803   0.526807  0.781746    NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>data</th>\n      <th>length_used</th>\n      <th>feature_extraction</th>\n      <th>feature_selection</th>\n      <th>model</th>\n      <th>accuracy</th>\n      <th>f1</th>\n      <th>recall</th>\n      <th>precision</th>\n      <th>roc_auc</th>\n      <th>notes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer()</td>\n      <td>NaN</td>\n      <td>LogisticRegression(max_iter=1000)</td>\n      <td>0.854472</td>\n      <td>0.854731</td>\n      <td>0.856315</td>\n      <td>0.853172</td>\n      <td>0.932805</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer()</td>\n      <td>NaN</td>\n      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n      <td>0.849030</td>\n      <td>0.848443</td>\n      <td>0.845242</td>\n      <td>0.851696</td>\n      <td>0.931995</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer()</td>\n      <td>NaN</td>\n      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n      <td>0.849030</td>\n      <td>0.848443</td>\n      <td>0.845242</td>\n      <td>0.851696</td>\n      <td>0.931995</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer()</td>\n      <td>NaN</td>\n      <td>LGBMClassifier(force_row_wise=True)</td>\n      <td>0.844801</td>\n      <td>0.843937</td>\n      <td>0.839496</td>\n      <td>0.848431</td>\n      <td>0.930118</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer()</td>\n      <td>NaN</td>\n      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n      <td>0.839112</td>\n      <td>0.836559</td>\n      <td>0.823757</td>\n      <td>0.849778</td>\n      <td>0.926102</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=100)</td>\n      <td>NaN</td>\n      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n      <td>0.803868</td>\n      <td>0.808183</td>\n      <td>0.826445</td>\n      <td>0.790737</td>\n      <td>0.891575</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=100)</td>\n      <td>NaN</td>\n      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n      <td>0.803868</td>\n      <td>0.808183</td>\n      <td>0.826445</td>\n      <td>0.790737</td>\n      <td>0.891575</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=100)</td>\n      <td>NaN</td>\n      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n      <td>0.799391</td>\n      <td>0.803480</td>\n      <td>0.820276</td>\n      <td>0.787402</td>\n      <td>0.888400</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=100)</td>\n      <td>NaN</td>\n      <td>LGBMClassifier(force_row_wise=True)</td>\n      <td>0.795978</td>\n      <td>0.796427</td>\n      <td>0.798369</td>\n      <td>0.794521</td>\n      <td>0.882648</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer()</td>\n      <td>NaN</td>\n      <td>AdaBoostClassifier(n_estimators=100)</td>\n      <td>0.779358</td>\n      <td>0.779414</td>\n      <td>0.779717</td>\n      <td>0.779169</td>\n      <td>0.870845</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=50)</td>\n      <td>NaN</td>\n      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n      <td>0.765483</td>\n      <td>0.770820</td>\n      <td>0.788956</td>\n      <td>0.753517</td>\n      <td>0.851934</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=50)</td>\n      <td>NaN</td>\n      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n      <td>0.765483</td>\n      <td>0.770820</td>\n      <td>0.788956</td>\n      <td>0.753517</td>\n      <td>0.851934</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=50)</td>\n      <td>NaN</td>\n      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n      <td>0.762317</td>\n      <td>0.767627</td>\n      <td>0.785162</td>\n      <td>0.750896</td>\n      <td>0.850886</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=50)</td>\n      <td>NaN</td>\n      <td>LGBMClassifier(force_row_wise=True)</td>\n      <td>0.752448</td>\n      <td>0.754085</td>\n      <td>0.759122</td>\n      <td>0.749139</td>\n      <td>0.837559</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=100)</td>\n      <td>NaN</td>\n      <td>AdaBoostClassifier(n_estimators=100)</td>\n      <td>0.749530</td>\n      <td>0.757824</td>\n      <td>0.783928</td>\n      <td>0.733445</td>\n      <td>0.832393</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=50)</td>\n      <td>NaN</td>\n      <td>AdaBoostClassifier(n_estimators=100)</td>\n      <td>0.717451</td>\n      <td>0.724684</td>\n      <td>0.743805</td>\n      <td>0.706553</td>\n      <td>0.793530</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=100)</td>\n      <td>NaN</td>\n      <td>LogisticRegression(max_iter=1000)</td>\n      <td>0.676049</td>\n      <td>0.678463</td>\n      <td>0.683620</td>\n      <td>0.673423</td>\n      <td>0.738315</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=10)</td>\n      <td>NaN</td>\n      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n      <td>0.634918</td>\n      <td>0.642519</td>\n      <td>0.656301</td>\n      <td>0.629349</td>\n      <td>0.688036</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=50)</td>\n      <td>NaN</td>\n      <td>LogisticRegression(max_iter=1000)</td>\n      <td>0.631950</td>\n      <td>0.635951</td>\n      <td>0.643011</td>\n      <td>0.629166</td>\n      <td>0.681856</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=50)</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier(metric='cosine')</td>\n      <td>0.630023</td>\n      <td>0.694295</td>\n      <td>0.846551</td>\n      <td>0.588672</td>\n      <td>0.703485</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=50)</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier(metric='cosine', n_neighb...</td>\n      <td>0.627733</td>\n      <td>0.693486</td>\n      <td>0.842666</td>\n      <td>0.589465</td>\n      <td>0.695421</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=100)</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier(metric='cosine', n_neighb...</td>\n      <td>0.624011</td>\n      <td>0.705586</td>\n      <td>0.901408</td>\n      <td>0.579871</td>\n      <td>0.712325</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=10)</td>\n      <td>NaN</td>\n      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n      <td>0.623936</td>\n      <td>0.633346</td>\n      <td>0.649673</td>\n      <td>0.617967</td>\n      <td>0.675575</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=10)</td>\n      <td>NaN</td>\n      <td>&lt;catboost.core.CatBoostClassifier object at 0x...</td>\n      <td>0.623936</td>\n      <td>0.633346</td>\n      <td>0.649673</td>\n      <td>0.617967</td>\n      <td>0.675575</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=50)</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier(metric='cosine', n_neighb...</td>\n      <td>0.622378</td>\n      <td>0.706701</td>\n      <td>0.909949</td>\n      <td>0.577719</td>\n      <td>0.758787</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=10)</td>\n      <td>NaN</td>\n      <td>LGBMClassifier(force_row_wise=True)</td>\n      <td>0.619806</td>\n      <td>0.626396</td>\n      <td>0.637486</td>\n      <td>0.615838</td>\n      <td>0.670927</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=50)</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier(metric='euclidean', n_nei...</td>\n      <td>0.618647</td>\n      <td>0.689355</td>\n      <td>0.852387</td>\n      <td>0.579229</td>\n      <td>0.693508</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=100)</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier(metric='cosine')</td>\n      <td>0.615247</td>\n      <td>0.695359</td>\n      <td>0.884462</td>\n      <td>0.572979</td>\n      <td>0.663069</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer()</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier(metric='cosine', n_neighb...</td>\n      <td>0.605881</td>\n      <td>0.707367</td>\n      <td>0.952705</td>\n      <td>0.562726</td>\n      <td>0.717157</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=100)</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier(metric='cosine', n_neighb...</td>\n      <td>0.605176</td>\n      <td>0.706435</td>\n      <td>0.950214</td>\n      <td>0.562239</td>\n      <td>0.777750</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer()</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier(metric='euclidean', n_nei...</td>\n      <td>0.604674</td>\n      <td>0.705149</td>\n      <td>0.951818</td>\n      <td>0.560207</td>\n      <td>0.716450</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=10)</td>\n      <td>NaN</td>\n      <td>AdaBoostClassifier(n_estimators=100)</td>\n      <td>0.604101</td>\n      <td>0.611954</td>\n      <td>0.624399</td>\n      <td>0.600114</td>\n      <td>0.646348</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=10)</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier(metric='cosine', n_neighb...</td>\n      <td>0.590794</td>\n      <td>0.606033</td>\n      <td>0.629740</td>\n      <td>0.584293</td>\n      <td>0.632423</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=100)</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier(metric='euclidean', n_nei...</td>\n      <td>0.580500</td>\n      <td>0.684094</td>\n      <td>0.913378</td>\n      <td>0.547310</td>\n      <td>0.676480</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=10)</td>\n      <td>NaN</td>\n      <td>LogisticRegression(max_iter=1000)</td>\n      <td>0.567818</td>\n      <td>0.580897</td>\n      <td>0.599094</td>\n      <td>0.563848</td>\n      <td>0.596594</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=10)</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier(metric='euclidean', n_nei...</td>\n      <td>0.560846</td>\n      <td>0.568853</td>\n      <td>0.584163</td>\n      <td>0.555935</td>\n      <td>0.579758</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=10)</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier(metric='cosine')</td>\n      <td>0.560096</td>\n      <td>0.564041</td>\n      <td>0.573705</td>\n      <td>0.556180</td>\n      <td>0.578870</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=10)</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier(metric='cosine', n_neighb...</td>\n      <td>0.556966</td>\n      <td>0.561606</td>\n      <td>0.568422</td>\n      <td>0.556642</td>\n      <td>0.574380</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer()</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier(metric='cosine', n_neighb...</td>\n      <td>0.549874</td>\n      <td>0.685645</td>\n      <td>0.981803</td>\n      <td>0.526807</td>\n      <td>0.781746</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}