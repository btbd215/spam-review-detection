{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KyThuat88\\AppData\\Local\\Temp\\ipykernel_13780\\4080736814.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('datacleaning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "      <td>love well made sturdi comfort love pretti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "      <td>love great upgrad origin mine coupl year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "      <td>pillow save back love look feel pillow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "      <td>miss inform use great product price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "      <td>nice set good qualiti set two month</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category  rating label  \\\n",
       "0  Home_and_Kitchen_5     5.0    CG   \n",
       "1  Home_and_Kitchen_5     5.0    CG   \n",
       "2  Home_and_Kitchen_5     5.0    CG   \n",
       "3  Home_and_Kitchen_5     1.0    CG   \n",
       "4  Home_and_Kitchen_5     5.0    CG   \n",
       "\n",
       "                                               text_  \\\n",
       "0  Love this!  Well made, sturdy, and very comfor...   \n",
       "1  love it, a great upgrade from the original.  I...   \n",
       "2  This pillow saved my back. I love the look and...   \n",
       "3  Missing information on how to use it, but it i...   \n",
       "4  Very nice set. Good quality. We have had the s...   \n",
       "\n",
       "                                        text  \n",
       "0  love well made sturdi comfort love pretti  \n",
       "1   love great upgrad origin mine coupl year  \n",
       "2     pillow save back love look feel pillow  \n",
       "3        miss inform use great product price  \n",
       "4        nice set good qualiti set two month  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KyThuat88\\AppData\\Local\\Temp\\ipykernel_13780\\1844944567.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['text'].fillna('',inplace = True)\n"
     ]
    }
   ],
   "source": [
    "data['text'].fillna('',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['length'] = data['text_'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "mapping = {'OR' : 0, 'CG' : 1}\n",
    "data['label'] = le.fit_transform(data['label'].map(mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score,precision_score,roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spam:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.split, self.X, self.y = self.train_test_split(data=data)\n",
    "\n",
    "    def train_test_split(self, data):\n",
    "        kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "        X = data[['text','length']]\n",
    "        y = data['label']\n",
    "        folds = list(kf.split(X))\n",
    "        return folds,X,y\n",
    "\n",
    "    def extraction(self, min_df=0.0, ngram=(1,1), X_train=None, X_test=None):\n",
    "        vectorizer = TfidfVectorizer(min_df=min_df, ngram_range=ngram)\n",
    "        X_trained = pd.DataFrame(vectorizer.fit_transform(X_train['text']).toarray(), columns = vectorizer.get_feature_names_out(), index=X_train.index)\n",
    "        X_tested = pd.DataFrame(vectorizer.transform(X_test['text']).toarray(), columns = vectorizer.get_feature_names_out(), index=X_test.index)\n",
    "        return X_trained, X_tested\n",
    "\n",
    "    def model_mapping(self, model):\n",
    "        mapping = {\n",
    "            'Logistics': LogisticRegression,\n",
    "            'SVM': SVC,\n",
    "            'KNN': KNeighborsClassifier,\n",
    "            'Naive Bayes': GaussianNB,\n",
    "            'MLP': MLPClassifier\n",
    "        }\n",
    "        return model, mapping[model]\n",
    "    def train_report(self, model, X_train=None, y_train=None, X_test=None, y_test=None,param=None):\n",
    "        clf = model(**param)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test,y_pred)\n",
    "        f1 = f1_score(y_test,y_pred)\n",
    "        precision = precision_score(y_test,y_pred)\n",
    "        y_pred_roc = clf.predict_proba(X_test)[:,1]\n",
    "        roc = roc_auc_score(y_test,y_pred_roc)\n",
    "        return accuracy,recall,f1,precision,roc\n",
    "    \n",
    "    def feature_selection(self,X_train = None,X_test = None, method = 'pca'):\n",
    "        if method == 'pca':\n",
    "            extract = PCA(n_components = 1000,random_state = 42)\n",
    "        else:\n",
    "            extract = SelectKBest(chi2, k=1000)\n",
    "        X_train = extract.fit_transform(X_train)\n",
    "        X_test = extract.transform(X_test)\n",
    "        return X_train,X_test\n",
    "        \n",
    "    def model_selection(self, model='Logistics', param =None,min_df=None, ngram=None, method = None):\n",
    "        accu = []\n",
    "        reca = []\n",
    "        f1s = []\n",
    "        pres = []\n",
    "        rocau = []\n",
    "        model_name, model_func = self.model_mapping(model)\n",
    "        method_used = 'none'\n",
    "        for train_index, test_index in self.split:\n",
    "            X_train, X_test = self.X.iloc[train_index], self.X.iloc[test_index]\n",
    "            y_train, y_test = self.y.iloc[train_index], self.y.iloc[test_index]\n",
    "\n",
    "            train_vec, test_vec = self.extraction(min_df=min_df,ngram=ngram,X_train=X_train, X_test=X_test)\n",
    "            scale = StandardScaler()\n",
    "            X_train = X_train.copy()\n",
    "            X_test = X_test.copy()\n",
    "            train_vec['og_length'] = scale.fit_transform(X_train[['length']])\n",
    "            test_vec['og_length'] = scale.transform(X_test[['length']])\n",
    "            if train_vec.shape[1] > 2000:\n",
    "                if method == 'pca':\n",
    "                    train_vec, test_vec = self.feature_selection(train_vec, test_vec, method='pca')\n",
    "                    method_used = 'pca'\n",
    "                elif method == 'selectkbest':\n",
    "                    train_vec, test_vec = self.feature_selection(train_vec, test_vec, method='selectkbest')\n",
    "                    method_used = 'selectkbest'\n",
    "            else:\n",
    "                method_used = 'none'\n",
    "            acc,rec,f1,pre,roc = self.train_report(model = model_func, X_train = train_vec, y_train=y_train, X_test=test_vec, y_test=y_test,param = param)\n",
    "            accu.append(acc)\n",
    "            reca.append(rec)\n",
    "            f1s.append(f1)\n",
    "            pres.append(pre)\n",
    "            rocau.append(roc)\n",
    "        accuracy = np.mean(accu)\n",
    "        recall = np.mean(reca)\n",
    "        f1_score = np.mean(f1s)\n",
    "        precision = np.mean(pres)\n",
    "        roc_auc = np.mean(rocau)\n",
    "        return method_used,accuracy,recall,f1_score,precision,roc_auc\n",
    "\n",
    "\n",
    "def main(model, data, min_df=None, ngram=None, param=None, method = None):\n",
    "    results_df = pd.DataFrame(columns=['feature_extraction','feature_selection','model', 'accuracy', 'recall', 'f1_score', 'precision', 'roc_auc'])\n",
    "\n",
    "    spam_instance = Spam(data=data)\n",
    "\n",
    "    if model == 'KNN':\n",
    "        knn_params = {\n",
    "            'n_neighbors': [1, 3, 5],\n",
    "            'metric': ['cosine', 'euclidean', 'manhattan']\n",
    "        }\n",
    "        for knn_param in product(knn_params['n_neighbors'], knn_params['metric']):\n",
    "            knn_param_dict = {'n_neighbors': knn_param[0], 'metric': knn_param[1]}\n",
    "            for min_df_val, ngram_val in product(min_df, ngram):\n",
    "                select, acc, rec, f1, pre, roc = spam_instance.model_selection(model=model, param=knn_param_dict, min_df=min_df_val, ngram=ngram_val)\n",
    "                model_str = f\"{model}{{{min_df_val},{ngram_val}}}\"\n",
    "                results_df_length = len(results_df)\n",
    "                results_df.loc[results_df_length] = [select, model_str, acc, rec, f1, pre, roc]\n",
    "    else:\n",
    "        for min_df_val, ngram_val in product(min_df, ngram):\n",
    "            select, acc, rec, f1, pre, roc = spam_instance.model_selection(model=model, param=param, min_df=min_df_val, ngram=ngram_val, method = method)\n",
    "            extraction_str = f\"TF_IDF{{{min_df_val},{ngram_val}}}\"\n",
    "            results_df_length = len(results_df)\n",
    "            results_df.loc[results_df_length] = [extraction_str,select, model, acc, rec, f1, pre, roc]\n",
    "            print(results_df.loc[results_df_length])\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KyThuat88\\AppData\\Local\\Temp\\ipykernel_13780\\551527684.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['text'].fillna('',inplace = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_extraction    TF_IDF{0.001,(1, 1)}\n",
      "feature_selection                      pca\n",
      "model                            Logistics\n",
      "accuracy                          0.849253\n",
      "recall                            0.841937\n",
      "f1_score                          0.848122\n",
      "precision                         0.854406\n",
      "roc_auc                           0.927268\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_extraction\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_selection\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, param \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 24\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_df_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mngram_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mselectkbest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([results_df, results], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(results_df)\n",
      "Cell \u001b[1;32mIn[8], line 109\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(model, data, min_df, ngram, param, method)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m min_df_val, ngram_val \u001b[38;5;129;01min\u001b[39;00m product(min_df, ngram):\n\u001b[1;32m--> 109\u001b[0m         select, acc, rec, f1, pre, roc \u001b[38;5;241m=\u001b[39m \u001b[43mspam_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_df_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mngram_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m         extraction_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF_IDF\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmin_df_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mngram_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m         results_df_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(results_df)\n",
      "Cell \u001b[1;32mIn[8], line 69\u001b[0m, in \u001b[0;36mSpam.model_selection\u001b[1;34m(self, model, param, min_df, ngram, method)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_vec\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2000\u001b[39m:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpca\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 69\u001b[0m         train_vec, test_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpca\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m         method_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpca\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselectkbest\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[1;32mIn[8], line 45\u001b[0m, in \u001b[0;36mSpam.feature_selection\u001b[1;34m(self, X_train, X_test, method)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     extract \u001b[38;5;241m=\u001b[39m SelectKBest(chi2, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mextract\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m X_test \u001b[38;5;241m=\u001b[39m extract\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_train,X_test\n",
      "File \u001b[1;32mc:\\Users\\KyThuat88\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 273\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    279\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\KyThuat88\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\KyThuat88\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:454\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 454\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m     U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KyThuat88\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:516\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_full(X, n_components)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_truncated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_svd_solver\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\KyThuat88\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:656\u001b[0m, in \u001b[0;36mPCA._fit_truncated\u001b[1;34m(self, X, n_components, svd_solver)\u001b[0m\n\u001b[0;32m    652\u001b[0m     U, Vt \u001b[38;5;241m=\u001b[39m svd_flip(U[:, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], Vt[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;66;03m# sign flipping is done inside\u001b[39;00m\n\u001b[1;32m--> 656\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[43mrandomized_svd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_oversamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_oversamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterated_power\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflip_sign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_ \u001b[38;5;241m=\u001b[39m n_samples\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_ \u001b[38;5;241m=\u001b[39m Vt\n",
      "File \u001b[1;32mc:\\Users\\KyThuat88\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KyThuat88\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\extmath.py:494\u001b[0m, in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transpose:\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;66;03m# this implementation is a bit faster with smaller shape[1]\u001b[39;00m\n\u001b[0;32m    492\u001b[0m     M \u001b[38;5;241m=\u001b[39m M\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m--> 494\u001b[0m Q \u001b[38;5;241m=\u001b[39m \u001b[43mrandomized_range_finder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_random\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpower_iteration_normalizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;66;03m# project M to the (k + p) dimensional space using the basis vectors\u001b[39;00m\n\u001b[0;32m    503\u001b[0m B \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m M\n",
      "File \u001b[1;32mc:\\Users\\KyThuat88\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\extmath.py:311\u001b[0m, in \u001b[0;36mrandomized_range_finder\u001b[1;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# Perform power iterations with Q to further 'imprint' the top\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# singular vectors of A in Q\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[1;32m--> 311\u001b[0m     Q, _ \u001b[38;5;241m=\u001b[39m normalizer(\u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m)\n\u001b[0;32m    312\u001b[0m     Q, _ \u001b[38;5;241m=\u001b[39m normalizer(A\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m Q)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# Sample the range of A using by linear projection of Q\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;66;03m# Extract an orthonormal basis\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    data=pd.read_csv('datacleaning.csv')\n",
    "    data['text'].fillna('',inplace = True)\n",
    "    data=data.copy()\n",
    "    data['length'] = data['text_'].apply(lambda x: len(x.split()))\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    mapping = {'OR' : 0, 'CG' : 1}\n",
    "    data['label'] = le.fit_transform(data['label'].map(mapping))\n",
    "    # check = Spam(data=data)\n",
    "\n",
    "    # Define parameter combinations for each model\n",
    "    params = {\n",
    "        'Logistics': {'max_iter': 1000, 'class_weight': 'balanced', 'random_state': 42}}\n",
    "        #'SVM': {'probability': True},\n",
    "        #'Naive Bayes':\n",
    "    min_df_values = [0.001]\n",
    "    ngram_values = [(1,1),(1,2),(1, 3),(2, 2),(2, 3),(3, 3)]\n",
    "\n",
    "    results_df = pd.DataFrame(columns=['feature_extraction','feature_selection','model', 'accuracy', 'recall', 'f1_score', 'precision', 'roc_auc'])\n",
    "\n",
    "    for model_name, param in params.items():\n",
    "        results = main(model=model_name, data=data, min_df=min_df_values, ngram=ngram_values, param=param,method = 'selectkbest')\n",
    "        results_df = pd.concat([results_df, results], ignore_index=True)\n",
    "    \n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['length_used'] = 'StandardScaler'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['data'] = 'fake_reviews_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df[['data','length_used','feature_extraction','feature_selection','model', 'accuracy', 'f1_score','recall', 'precision', 'roc_auc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_df.to_csv('mindf_0.001_0.01_pca_logistics.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>length_used</th>\n",
       "      <th>feature_extraction</th>\n",
       "      <th>feature_selection</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fake_reviews_dataset</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>TF_IDF{0.01,(1, 1)}</td>\n",
       "      <td>none</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>0.811560</td>\n",
       "      <td>0.812744</td>\n",
       "      <td>0.818152</td>\n",
       "      <td>0.807415</td>\n",
       "      <td>0.894533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fake_reviews_dataset</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>TF_IDF{0.01,(1, 2)}</td>\n",
       "      <td>none</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>0.825707</td>\n",
       "      <td>0.825884</td>\n",
       "      <td>0.827122</td>\n",
       "      <td>0.824679</td>\n",
       "      <td>0.909541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fake_reviews_dataset</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>TF_IDF{0.01,(1, 3)}</td>\n",
       "      <td>none</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>0.826276</td>\n",
       "      <td>0.826314</td>\n",
       "      <td>0.826918</td>\n",
       "      <td>0.825732</td>\n",
       "      <td>0.909708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fake_reviews_dataset</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>TF_IDF{0.01,(2, 2)}</td>\n",
       "      <td>none</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>0.672141</td>\n",
       "      <td>0.620810</td>\n",
       "      <td>0.536804</td>\n",
       "      <td>0.736009</td>\n",
       "      <td>0.750723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fake_reviews_dataset</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>TF_IDF{0.01,(2, 3)}</td>\n",
       "      <td>none</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>0.672289</td>\n",
       "      <td>0.619996</td>\n",
       "      <td>0.534723</td>\n",
       "      <td>0.737649</td>\n",
       "      <td>0.752459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fake_reviews_dataset</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>TF_IDF{0.01,(3, 3)}</td>\n",
       "      <td>none</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>0.591462</td>\n",
       "      <td>0.591582</td>\n",
       "      <td>0.591949</td>\n",
       "      <td>0.591427</td>\n",
       "      <td>0.634581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fake_reviews_dataset</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>TF_IDF{0.001,(1, 1)}</td>\n",
       "      <td>pca</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>0.849253</td>\n",
       "      <td>0.848122</td>\n",
       "      <td>0.841937</td>\n",
       "      <td>0.854406</td>\n",
       "      <td>0.927268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fake_reviews_dataset</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>TF_IDF{0.001,(1, 2)}</td>\n",
       "      <td>pca</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>0.872304</td>\n",
       "      <td>0.871436</td>\n",
       "      <td>0.865889</td>\n",
       "      <td>0.877083</td>\n",
       "      <td>0.946797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fake_reviews_dataset</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>TF_IDF{0.001,(1, 3)}</td>\n",
       "      <td>pca</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>0.874555</td>\n",
       "      <td>0.873539</td>\n",
       "      <td>0.866807</td>\n",
       "      <td>0.880395</td>\n",
       "      <td>0.948573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fake_reviews_dataset</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>TF_IDF{0.001,(2, 2)}</td>\n",
       "      <td>pca</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>0.806367</td>\n",
       "      <td>0.796963</td>\n",
       "      <td>0.759895</td>\n",
       "      <td>0.837842</td>\n",
       "      <td>0.885487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fake_reviews_dataset</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>TF_IDF{0.001,(2, 3)}</td>\n",
       "      <td>pca</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>0.808988</td>\n",
       "      <td>0.799281</td>\n",
       "      <td>0.760541</td>\n",
       "      <td>0.842219</td>\n",
       "      <td>0.887750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>fake_reviews_dataset</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>TF_IDF{0.001,(3, 3)}</td>\n",
       "      <td>none</td>\n",
       "      <td>Logistics</td>\n",
       "      <td>0.710155</td>\n",
       "      <td>0.626243</td>\n",
       "      <td>0.485664</td>\n",
       "      <td>0.881511</td>\n",
       "      <td>0.795968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    data     length_used    feature_extraction  \\\n",
       "0   fake_reviews_dataset  StandardScaler   TF_IDF{0.01,(1, 1)}   \n",
       "1   fake_reviews_dataset  StandardScaler   TF_IDF{0.01,(1, 2)}   \n",
       "2   fake_reviews_dataset  StandardScaler   TF_IDF{0.01,(1, 3)}   \n",
       "3   fake_reviews_dataset  StandardScaler   TF_IDF{0.01,(2, 2)}   \n",
       "4   fake_reviews_dataset  StandardScaler   TF_IDF{0.01,(2, 3)}   \n",
       "5   fake_reviews_dataset  StandardScaler   TF_IDF{0.01,(3, 3)}   \n",
       "6   fake_reviews_dataset  StandardScaler  TF_IDF{0.001,(1, 1)}   \n",
       "7   fake_reviews_dataset  StandardScaler  TF_IDF{0.001,(1, 2)}   \n",
       "8   fake_reviews_dataset  StandardScaler  TF_IDF{0.001,(1, 3)}   \n",
       "9   fake_reviews_dataset  StandardScaler  TF_IDF{0.001,(2, 2)}   \n",
       "10  fake_reviews_dataset  StandardScaler  TF_IDF{0.001,(2, 3)}   \n",
       "11  fake_reviews_dataset  StandardScaler  TF_IDF{0.001,(3, 3)}   \n",
       "\n",
       "   feature_selection      model  accuracy  f1_score    recall  precision  \\\n",
       "0               none  Logistics  0.811560  0.812744  0.818152   0.807415   \n",
       "1               none  Logistics  0.825707  0.825884  0.827122   0.824679   \n",
       "2               none  Logistics  0.826276  0.826314  0.826918   0.825732   \n",
       "3               none  Logistics  0.672141  0.620810  0.536804   0.736009   \n",
       "4               none  Logistics  0.672289  0.619996  0.534723   0.737649   \n",
       "5               none  Logistics  0.591462  0.591582  0.591949   0.591427   \n",
       "6                pca  Logistics  0.849253  0.848122  0.841937   0.854406   \n",
       "7                pca  Logistics  0.872304  0.871436  0.865889   0.877083   \n",
       "8                pca  Logistics  0.874555  0.873539  0.866807   0.880395   \n",
       "9                pca  Logistics  0.806367  0.796963  0.759895   0.837842   \n",
       "10               pca  Logistics  0.808988  0.799281  0.760541   0.842219   \n",
       "11              none  Logistics  0.710155  0.626243  0.485664   0.881511   \n",
       "\n",
       "     roc_auc  \n",
       "0   0.894533  \n",
       "1   0.909541  \n",
       "2   0.909708  \n",
       "3   0.750723  \n",
       "4   0.752459  \n",
       "5   0.634581  \n",
       "6   0.927268  \n",
       "7   0.946797  \n",
       "8   0.948573  \n",
       "9   0.885487  \n",
       "10  0.887750  \n",
       "11  0.795968  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
