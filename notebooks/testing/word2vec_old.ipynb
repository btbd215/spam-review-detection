{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for local import\n",
    "import sys\n",
    "if \"../../\" not in sys.path:\n",
    "    sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for working with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "# for preprocessing\n",
    "from src.preprocessing import BasicTextCleaning\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# for modelling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# for evaluation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of OFS:  (40432, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40427</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>I had read some reviews saying that this bra r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40428</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>I wasn't sure exactly what it would be. It is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40429</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>You can wear the hood by itself, wear it with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40430</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>I liked nothing about this dress. The only rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40431</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>I work in the wedding industry and have to wor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40432 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           category  rating label  \\\n",
       "0                Home_and_Kitchen_5     5.0    CG   \n",
       "1                Home_and_Kitchen_5     5.0    CG   \n",
       "2                Home_and_Kitchen_5     5.0    CG   \n",
       "3                Home_and_Kitchen_5     1.0    CG   \n",
       "4                Home_and_Kitchen_5     5.0    CG   \n",
       "...                             ...     ...   ...   \n",
       "40427  Clothing_Shoes_and_Jewelry_5     4.0    OR   \n",
       "40428  Clothing_Shoes_and_Jewelry_5     5.0    CG   \n",
       "40429  Clothing_Shoes_and_Jewelry_5     2.0    OR   \n",
       "40430  Clothing_Shoes_and_Jewelry_5     1.0    CG   \n",
       "40431  Clothing_Shoes_and_Jewelry_5     5.0    OR   \n",
       "\n",
       "                                                   text_  \n",
       "0      Love this!  Well made, sturdy, and very comfor...  \n",
       "1      love it, a great upgrade from the original.  I...  \n",
       "2      This pillow saved my back. I love the look and...  \n",
       "3      Missing information on how to use it, but it i...  \n",
       "4      Very nice set. Good quality. We have had the s...  \n",
       "...                                                  ...  \n",
       "40427  I had read some reviews saying that this bra r...  \n",
       "40428  I wasn't sure exactly what it would be. It is ...  \n",
       "40429  You can wear the hood by itself, wear it with ...  \n",
       "40430  I liked nothing about this dress. The only rea...  \n",
       "40431  I work in the wedding industry and have to wor...  \n",
       "\n",
       "[40432 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osf = pd.read_csv(\"../../data/fake_reviews_dataset.csv\")\n",
    "print(\"Shape of OFS: \", osf.shape)\n",
    "# osf.head()\n",
    "osf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "count = osf['label'].value_counts()\n",
    "count\n",
    "sns.barplot(x=count.index, y=count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = BasicTextCleaning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    osf_cleaned = pd.read_csv(\"../../data/cleaned/osf_cleaned.csv\")\n",
    "    osf_cleaned = osf_cleaned.replace(np.nan, '')\n",
    "except:\n",
    "    osf_cleaned = pd.DataFrame()\n",
    "    osf_cleaned['length'] = osf['text_'].apply(lambda x: len(x))\n",
    "    osf_cleaned['texts'] = cleaner.text_cleaning(osf['text_'])\n",
    "\n",
    "    ordinal = OrdinalEncoder(categories=[['OR', 'CG']], dtype=int)\n",
    "    osf_cleaned['labels'] = ordinal.fit_transform(osf[['label']])\n",
    "    osf_cleaned.to_csv(\"../../data/cleaned/osf_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 300)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "class AvgWord2Vec:\n",
    "    def __init__(self, vector_size=300, min_count=1, sg=1, ngram_range=(1, 1), window=5, seed=42):\n",
    "        self.w2v = Word2Vec(vector_size=vector_size, min_count=min_count, sg=sg,\n",
    "                            window=window, workers=5, seed=seed)\n",
    "        self.min_count = min_count\n",
    "        self.sg = sg\n",
    "        self.window = window\n",
    "        self.seed = seed\n",
    "        self.vsize = vector_size\n",
    "\n",
    "        self.ngrams = np.arange(ngram_range[0], ngram_range[1]+1, 1)\n",
    "        self.raw = None\n",
    "        self.corpus = None\n",
    "        self.vocabulary_ = None\n",
    "\n",
    "    def _create_ngrams(self, n, X):\n",
    "        phrases = [list(ngrams(sent.split(), n)) for sent in X]\n",
    "        for i in range(len(phrases)):\n",
    "            phrases[i] = [\" \".join(word) for word in phrases[i]]\n",
    "        return phrases\n",
    "    \n",
    "    def _create_corpus(self, X, update_train=False):\n",
    "        ngrams_phrases = {}\n",
    "        for n in self.ngrams:\n",
    "            phrases = self._create_ngrams(n, X)\n",
    "            ngrams_phrases[f\"{n}\"] = phrases\n",
    "        data = []\n",
    "        corpus = []\n",
    "        for n in ngrams_phrases.values():\n",
    "            if len(data)==0:\n",
    "                data = n\n",
    "            data = [data[i] + n[i] for i in range(len(data))]\n",
    "            corpus.extend(n)\n",
    "        if update_train:\n",
    "            self.corpus = corpus\n",
    "        return data\n",
    "    \n",
    "    def _avg_sentence(self, data):\n",
    "        avg_sentences = []\n",
    "        for sent in data:\n",
    "            if len(sent)!=0:\n",
    "                avg_sentence = np.mean([self.w2v.wv.get_vector(word) for word in sent\n",
    "                                        if word in self.w2v.wv.index_to_key], axis=0)\n",
    "            else:\n",
    "                avg_sentence = np.zeros(self.vsize)\n",
    "            avg_sentences.append(avg_sentence)\n",
    "        return np.array(avg_sentences)\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.w2v = Word2Vec(vector_size=self.vsize, min_count=self.min_count, sg=self.sg,\n",
    "                            window=self.window, workers=5, seed=self.seed)\n",
    "        self.raw = list(X)\n",
    "        self._create_corpus(update_train=True, X=X)\n",
    "        self.w2v.build_vocab(self.corpus)\n",
    "        self.w2v.train(self.corpus, total_examples=self.w2v.corpus_count, epochs=self.w2v.epochs)\n",
    "        self.vocabulary_ = self.w2v.wv.key_to_index\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.w2v = Word2Vec(vector_size=self.vsize, min_count=self.min_count, sg=self.sg,\n",
    "                            window=self.window, workers=5, seed=self.seed)\n",
    "        self.raw = list(X)\n",
    "        data = self._create_corpus(update_train=True, X=X)\n",
    "        self.w2v.build_vocab(self.corpus)\n",
    "        self.w2v.train(self.corpus, total_examples=self.w2v.corpus_count, epochs=self.w2v.epochs)\n",
    "        self.vocabulary_ = self.w2v.wv.key_to_index\n",
    "        return scipy.sparse.csr_matrix(self._avg_sentence(data))\n",
    "        \n",
    "    def transform(self, X):\n",
    "        data = self._create_corpus(update_train=False, X=X)\n",
    "        return scipy.sparse.csr_matrix(self._avg_sentence(data))\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        columns = np.array([f'component_{i+1}' for i in range(self.vsize)])\n",
    "\n",
    "texts = [\"I love eating chocolate icecream and strawberry cake\",\n",
    "         \"Dogs are obedient\"]\n",
    "y = [\"julia nice\",\n",
    "     \"bibimbap\"]\n",
    "x = [\"i love you\",\n",
    "     \"beautiful girl\"]\n",
    "\n",
    "w2v = AvgWord2Vec(ngram_range=(1, 1))\n",
    "w2v.fit_transform(x).toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<40432x28217 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 993426 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(osf_cleaned['texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0005', ..., 'zurg', 'zyliss', 'zymox'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word2vec(sentences, w2v_model):\n",
    "    avg_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence:\n",
    "            avg_sentence = np.mean([w2v_model.wv.get_vector(word) for word in sentence if word in w2v_model.wv.key_to_index], axis=0)\n",
    "        else:\n",
    "            avg_sentence = np.zeros(w2v_model.vector_size)\n",
    "        avg_sentences.append(avg_sentence)\n",
    "    return np.array(avg_sentences)\n",
    "\n",
    "def text_extractor(X_train, X_test, extractor):\n",
    "    if isinstance(extractor, Word2Vec):\n",
    "        vector_size = extractor.vector_size\n",
    "        window = extractor.window\n",
    "        sg = extractor.sg\n",
    "        extractor = Word2Vec(vector_size=vector_size, sg=sg, window=window, min_count=1, workers=5, seed=42)\n",
    "\n",
    "        cleaner = BasicTextCleaning()\n",
    "        X_train = cleaner.text_cleaning(texts=X_train, methods=['tokenization'])\n",
    "        X_test = cleaner.text_cleaning(texts=X_test, methods=['tokenization'])\n",
    "\n",
    "        extractor.build_vocab(X_train)\n",
    "        extractor.train(X_train, total_examples=extractor.corpus_count, epochs=30)\n",
    "        X_train = avg_word2vec(X_train, extractor)\n",
    "        X_train = pd.DataFrame(X_train, columns=[str(i) for i in range(extractor.vector_size)])\n",
    "        X_test = avg_word2vec(X_test, extractor)\n",
    "        X_test = pd.DataFrame(X_test, columns=[str(i) for i in range(extractor.vector_size)])\n",
    "    else:\n",
    "        X_train = extractor.fit_transform(X_train).toarray()\n",
    "        X_test = extractor.transform(X_test).toarray()\n",
    "        X_train = pd.DataFrame(X_train, columns=extractor.get_feature_names_out())\n",
    "        X_test = pd.DataFrame(X_test, columns=extractor.get_feature_names_out())\n",
    "    \n",
    "    variance = VarianceThreshold()\n",
    "    X_train = variance.fit_transform(X_train)\n",
    "    X_test = variance.transform(X_test)\n",
    "    X_train = pd.DataFrame(X_train, columns=variance.get_feature_names_out())\n",
    "    X_test = pd.DataFrame(X_test, columns=variance.get_feature_names_out())\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "def text_extractor(X_train, X_test, extractor):\n",
    "    # if isinstance(extractor, Word2Vec):\n",
    "    #     vector_size = extractor.vector_size\n",
    "    #     window = extractor.window\n",
    "    #     sg = extractor.sg\n",
    "    #     extractor = Word2Vec(vector_size=vector_size, sg=sg, window=window, min_count=1, workers=5, seed=42)\n",
    "\n",
    "    #     cleaner = BasicTextCleaning()\n",
    "    #     X_train = cleaner.text_cleaning(texts=X_train, methods=['tokenization'])\n",
    "    #     X_test = cleaner.text_cleaning(texts=X_test, methods=['tokenization'])\n",
    "\n",
    "    #     extractor.build_vocab(X_train)\n",
    "    #     extractor.train(X_train, total_examples=extractor.corpus_count, epochs=30)\n",
    "    #     X_train = avg_word2vec(X_train, extractor)\n",
    "    #     X_train = pd.DataFrame(X_train, columns=[str(i) for i in range(extractor.vector_size)])\n",
    "    #     X_test = avg_word2vec(X_test, extractor)\n",
    "    #     X_test = pd.DataFrame(X_test, columns=[str(i) for i in range(extractor.vector_size)])\n",
    "    # else:\n",
    "    X_train = extractor.fit_transform(X_train).toarray()\n",
    "    X_test = extractor.transform(X_test).toarray()\n",
    "    X_train = pd.DataFrame(X_train, columns=extractor.get_feature_names_out())\n",
    "    X_test = pd.DataFrame(X_test, columns=extractor.get_feature_names_out())\n",
    "    \n",
    "    variance = VarianceThreshold()\n",
    "    X_train = variance.fit_transform(X_train)\n",
    "    X_test = variance.transform(X_test)\n",
    "    X_train = pd.DataFrame(X_train, columns=variance.get_feature_names_out())\n",
    "    X_test = pd.DataFrame(X_test, columns=variance.get_feature_names_out())\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "def feature_selection(X_train, X_test, selector):\n",
    "    X_train = selector.fit_transform(X_train)\n",
    "    X_test = selector.transform(X_test)\n",
    "    X_train = pd.DataFrame(X_train, columns=selector.get_feature_names_out())\n",
    "    X_test = pd.DataFrame(X_test, columns=selector.get_feature_names_out())\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(model, X_train, y_train, X_test, probability=True):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    if probability:\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        return y_pred, y_pred_proba\n",
    "    return y_pred\n",
    "\n",
    "def evaluation(y_true, y_pred, y_pred_prob, scoring=['accuracy', 'f1', 'recall', 'precision', 'roc_auc']):\n",
    "    scores = {'accuracy': accuracy_score,\n",
    "              'f1': f1_score,\n",
    "              'recall': recall_score,\n",
    "              'precision': precision_score,\n",
    "              'roc_auc': roc_auc_score}\n",
    "    \n",
    "    result = {}\n",
    "    for method in scoring:\n",
    "        if method == 'roc_auc':\n",
    "            result[method] = scores[method](y_true, y_pred_prob.T[1])\n",
    "        else:\n",
    "            result[method] = scores[method](y_true, y_pred)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(data, extractor, model=None, selector=None, length_scaler=None,\n",
    "                     scoring=['accuracy', 'f1', 'recall', 'precision', 'roc_auc'], cv=5,\n",
    "                     avg_output=True, quiet=True):\n",
    "    kfolds = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    scores = {method: [] for method in scoring}\n",
    "    round = 1\n",
    "\n",
    "    for train_indices, test_indices in kfolds.split(data.iloc[:, :-1], data.iloc[:, -1]):\n",
    "        train_set, test_set = data.iloc[train_indices, :-1], data.iloc[test_indices, :-1]\n",
    "        y_train, y_test = data.iloc[train_indices, -1], data.iloc[test_indices, -1]\n",
    "\n",
    "        X_train, X_test = text_extractor(X_train=train_set['texts'], X_test=test_set['texts'], extractor=extractor)\n",
    "\n",
    "        if length_scaler is not None:\n",
    "            X_train['length'] = length_scaler.fit_transform(train_set[['length']])\n",
    "            X_test['length'] = length_scaler.transform(test_set[['length']])\n",
    "        \n",
    "        if selector is not None:\n",
    "            X_train, X_test = feature_selection(X_train, X_test, selector)\n",
    "\n",
    "        y_pred, y_pred_prob = modelling(model, X_train, y_train, X_test)\n",
    "\n",
    "        result = evaluation(y_true=y_test, y_pred=y_pred, y_pred_prob=y_pred_prob, scoring=scoring)\n",
    "        for method in scoring:\n",
    "            scores[method].append(result[method])\n",
    "            \n",
    "        if not quiet:\n",
    "            print(f\"round {round}: done\")\n",
    "\n",
    "        round += 1\n",
    "        \n",
    "    if avg_output:\n",
    "        avg_scores = {key: np.mean(values) for key, values in scores.items()}\n",
    "\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1: done\n",
      "round 2: done\n",
      "round 3: done\n",
      "round 4: done\n",
      "round 5: done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8157894326358898,\n",
       " 'f1': 0.8178101616385056,\n",
       " 'recall': 0.8270010232539085,\n",
       " 'precision': 0.8088330039753417,\n",
       " 'roc_auc': 0.87974034344771}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor = AvgWord2Vec(window=2, vector_size=50, seed=42, sg=0)\n",
    "model = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "# model = SVC(probability=True, class_weight='balanced')\n",
    "# extractor = TfidfVectorizer(min_df=0.001, ngram_range=(1, 1))\n",
    "# model = SVC()\n",
    "\n",
    "cross_validation(data=osf_cleaned, length_scaler=StandardScaler(),\n",
    "                 model=model, extractor=extractor, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcases = {'data': {'osf': osf_cleaned},\n",
    "             'length_used': {'None': None,\n",
    "                             'StandardScaler': StandardScaler(),\n",
    "                             'MinMaxScaler': MinMaxScaler()},\n",
    "             'feature_extraction': ['Word2Vec(vector_size={}, window={})'],\n",
    "             'feature selection': {'None': None,\n",
    "                                   'PCA': PCA,\n",
    "                                   'SelectKBest(score_func={}, k={})': SelectKBest},\n",
    "             'model': {'LogisticRegression(max_iter=1000, class_weight=\"balanced\")': LogisticRegression(max_iter=1000, class_weight='balanced')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {'data': [],\n",
    "          'length_used': [],\n",
    "          'feature_extraction': [],\n",
    "          'feature_selection': [],\n",
    "          'model': [],\n",
    "          'accuracy': [],\n",
    "          'f1': [],\n",
    "          'recall': [],\n",
    "          'precision': [],\n",
    "          'roc_auc': [],\n",
    "          'notes': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = {'data': [],\n",
    "#           'length_used': [],\n",
    "#           'feature_extraction': [],\n",
    "#           'feature_selection': [],\n",
    "#           'model': [],\n",
    "#           'accuracy': [],\n",
    "#           'f1': [],\n",
    "#           'recall': [],\n",
    "#           'precision': [],\n",
    "#           'roc_auc': [],\n",
    "#           'notes': []}\n",
    "\n",
    "# testcases = {'data': {'osf': osf_cleaned},\n",
    "#              'length_used': {'None': None,\n",
    "#                              'StandardScaler': StandardScaler(),\n",
    "#                              'MinMaxScaler': MinMaxScaler()},\n",
    "#              'feature_extraction': ['Word2Vec(vector_size={}, window={})'],\n",
    "#              'feature selection': {'None': None},\n",
    "#              'model': {'LogisticRegression(max_iter=1000, class_weight=\"balanced\")': LogisticRegression(max_iter=1000, class_weight='balanced')}}\n",
    "\n",
    "# for data_name in testcases['data']:\n",
    "#     for length in testcases['length_used']:\n",
    "#         if length == 'None':\n",
    "#             data = testcases['data'][data_name].iloc[:, 1:].copy()\n",
    "#         else:\n",
    "#             data = testcases['data'][data_name].copy()\n",
    "#         for size in np.arange(100, 1100, 100):\n",
    "#             for window in range(3, 9, 2):\n",
    "#                 extractor = Word2Vec(vector_size=size, window=window, workers=5, min_count=1, seed=42)\n",
    "#                 for selector_name in testcases['feature selection']:\n",
    "#                     selector = testcases['feature selection'][selector_name]\n",
    "#                     for model_name in testcases['model']:\n",
    "#                         model = testcases['model'][model_name]\n",
    "#                         scores = cross_validation(data=data,\n",
    "#                                                   length_scaler=testcases['length_used'][length],\n",
    "#                                                   extractor=extractor,\n",
    "#                                                   selector=selector,\n",
    "#                                                   model=model)\n",
    "                        \n",
    "#                         output['data'].append(data_name)\n",
    "#                         output['length_used'].append(length)\n",
    "#                         output['feature_extraction'].append(f'Word2Vec(vector_size={size}, window={window})')\n",
    "#                         output['feature_selection'].append(selector_name)\n",
    "#                         output['model'].append(model_name)\n",
    "#                         for key, values in scores.items():\n",
    "#                             output[key].append(values)\n",
    "\n",
    "output = {'data': [],\n",
    "          'length_used': [],\n",
    "          'feature_extraction': [],\n",
    "          'feature_selection': [],\n",
    "          'model': [],\n",
    "          'accuracy': [],\n",
    "          'f1': [],\n",
    "          'recall': [],\n",
    "          'precision': [],\n",
    "          'roc_auc': [],\n",
    "          'notes': []}\n",
    "\n",
    "testcases = {'data': {'osf': osf_cleaned},\n",
    "             'length_used': {'None': None},\n",
    "             'feature_extraction': ['Word2Vec(vector_size={}, window={})'],\n",
    "             'feature selection': {'None': None},\n",
    "             'model': {'LogisticRegression(max_iter=1000, class_weight=\"balanced\")': LogisticRegression(max_iter=1000, class_weight='balanced')}}\n",
    "\n",
    "for data_name in testcases['data']:\n",
    "    for length in testcases['length_used']:\n",
    "        if length == 'None':\n",
    "            data = testcases['data'][data_name].iloc[:, 1:].copy()\n",
    "        else:\n",
    "            data = testcases['data'][data_name].copy()\n",
    "        for size in np.arange(700, 710, 100):\n",
    "            for window in range(5, 9, 2):\n",
    "                extractor = Word2Vec(vector_size=size, window=window, workers=5, min_count=1, seed=42, sg=1)\n",
    "                for selector_name in testcases['feature selection']:\n",
    "                    selector = testcases['feature selection'][selector_name]\n",
    "                    for model_name in testcases['model']:\n",
    "                        model = testcases['model'][model_name]\n",
    "                        scores = cross_validation(data=data,\n",
    "                                                  length_scaler=testcases['length_used'][length],\n",
    "                                                  extractor=extractor,\n",
    "                                                  selector=selector,\n",
    "                                                  model=model)\n",
    "                        \n",
    "                        output['data'].append(data_name)\n",
    "                        output['length_used'].append(length)\n",
    "                        output['feature_extraction'].append(f'Word2Vec(vector_size={size}, window={window}, sg=1)')\n",
    "                        output['feature_selection'].append(selector_name)\n",
    "                        output['model'].append(model_name)\n",
    "                        for key, values in scores.items():\n",
    "                            output[key].append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_saved = output.copy()\n",
    "output_saved['notes'] = ['']*len(output_saved['data'])\n",
    "# # pd.DataFrame(output_saved).to_csv(\"data/result/test_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>length_used</th>\n",
       "      <th>feature_extraction</th>\n",
       "      <th>feature_selection</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>osf</td>\n",
       "      <td>None</td>\n",
       "      <td>Word2Vec(vector_size=700, window=5, sg=1)</td>\n",
       "      <td>None</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.864736</td>\n",
       "      <td>0.865342</td>\n",
       "      <td>0.869477</td>\n",
       "      <td>0.861249</td>\n",
       "      <td>0.926651</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>osf</td>\n",
       "      <td>None</td>\n",
       "      <td>Word2Vec(vector_size=700, window=7, sg=1)</td>\n",
       "      <td>None</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.863821</td>\n",
       "      <td>0.864472</td>\n",
       "      <td>0.868847</td>\n",
       "      <td>0.860146</td>\n",
       "      <td>0.926498</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data length_used                         feature_extraction  \\\n",
       "0  osf        None  Word2Vec(vector_size=700, window=5, sg=1)   \n",
       "1  osf        None  Word2Vec(vector_size=700, window=7, sg=1)   \n",
       "\n",
       "  feature_selection                                              model  \\\n",
       "0              None  LogisticRegression(max_iter=1000, class_weight...   \n",
       "1              None  LogisticRegression(max_iter=1000, class_weight...   \n",
       "\n",
       "   accuracy        f1    recall  precision   roc_auc notes  \n",
       "0  0.864736  0.865342  0.869477   0.861249  0.926651        \n",
       "1  0.863821  0.864472  0.868847   0.860146  0.926498        "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_saved_df = pd.DataFrame(output_saved)\n",
    "output_saved_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>length_used</th>\n",
       "      <th>feature_extraction</th>\n",
       "      <th>feature_selection</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=100, window=3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.842649</td>\n",
       "      <td>0.843850</td>\n",
       "      <td>0.850587</td>\n",
       "      <td>0.837234</td>\n",
       "      <td>0.906563</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=100, window=5)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.847769</td>\n",
       "      <td>0.848826</td>\n",
       "      <td>0.855068</td>\n",
       "      <td>0.842681</td>\n",
       "      <td>0.911585</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=100, window=7)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.849649</td>\n",
       "      <td>0.850687</td>\n",
       "      <td>0.856728</td>\n",
       "      <td>0.844732</td>\n",
       "      <td>0.913208</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=200, window=3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.852518</td>\n",
       "      <td>0.853635</td>\n",
       "      <td>0.860505</td>\n",
       "      <td>0.846878</td>\n",
       "      <td>0.915526</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=200, window=5)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.854348</td>\n",
       "      <td>0.855324</td>\n",
       "      <td>0.861271</td>\n",
       "      <td>0.849461</td>\n",
       "      <td>0.917639</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=600, window=5, sg=1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.863796</td>\n",
       "      <td>0.864541</td>\n",
       "      <td>0.869531</td>\n",
       "      <td>0.859616</td>\n",
       "      <td>0.925525</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=600, window=7, sg=1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.863029</td>\n",
       "      <td>0.863597</td>\n",
       "      <td>0.867322</td>\n",
       "      <td>0.859918</td>\n",
       "      <td>0.924754</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=700, window=3, sg=1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.863326</td>\n",
       "      <td>0.864018</td>\n",
       "      <td>0.868673</td>\n",
       "      <td>0.859418</td>\n",
       "      <td>0.925477</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>osf</td>\n",
       "      <td>None</td>\n",
       "      <td>Word2Vec(vector_size=700, window=5, sg=1)</td>\n",
       "      <td>None</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.864736</td>\n",
       "      <td>0.865342</td>\n",
       "      <td>0.869477</td>\n",
       "      <td>0.861249</td>\n",
       "      <td>0.926651</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>osf</td>\n",
       "      <td>None</td>\n",
       "      <td>Word2Vec(vector_size=700, window=7, sg=1)</td>\n",
       "      <td>None</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.863821</td>\n",
       "      <td>0.864472</td>\n",
       "      <td>0.868847</td>\n",
       "      <td>0.860146</td>\n",
       "      <td>0.926498</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    data length_used                         feature_extraction  \\\n",
       "0    osf         NaN        Word2Vec(vector_size=100, window=3)   \n",
       "1    osf         NaN        Word2Vec(vector_size=100, window=5)   \n",
       "2    osf         NaN        Word2Vec(vector_size=100, window=7)   \n",
       "3    osf         NaN        Word2Vec(vector_size=200, window=3)   \n",
       "4    osf         NaN        Word2Vec(vector_size=200, window=5)   \n",
       "..   ...         ...                                        ...   \n",
       "106  osf         NaN  Word2Vec(vector_size=600, window=5, sg=1)   \n",
       "107  osf         NaN  Word2Vec(vector_size=600, window=7, sg=1)   \n",
       "108  osf         NaN  Word2Vec(vector_size=700, window=3, sg=1)   \n",
       "109  osf        None  Word2Vec(vector_size=700, window=5, sg=1)   \n",
       "110  osf        None  Word2Vec(vector_size=700, window=7, sg=1)   \n",
       "\n",
       "    feature_selection                                              model  \\\n",
       "0                 NaN  LogisticRegression(max_iter=1000, class_weight...   \n",
       "1                 NaN  LogisticRegression(max_iter=1000, class_weight...   \n",
       "2                 NaN  LogisticRegression(max_iter=1000, class_weight...   \n",
       "3                 NaN  LogisticRegression(max_iter=1000, class_weight...   \n",
       "4                 NaN  LogisticRegression(max_iter=1000, class_weight...   \n",
       "..                ...                                                ...   \n",
       "106               NaN  LogisticRegression(max_iter=1000, class_weight...   \n",
       "107               NaN  LogisticRegression(max_iter=1000, class_weight...   \n",
       "108               NaN  LogisticRegression(max_iter=1000, class_weight...   \n",
       "109              None  LogisticRegression(max_iter=1000, class_weight...   \n",
       "110              None  LogisticRegression(max_iter=1000, class_weight...   \n",
       "\n",
       "     accuracy        f1    recall  precision   roc_auc notes  \n",
       "0    0.842649  0.843850  0.850587   0.837234  0.906563   NaN  \n",
       "1    0.847769  0.848826  0.855068   0.842681  0.911585   NaN  \n",
       "2    0.849649  0.850687  0.856728   0.844732  0.913208   NaN  \n",
       "3    0.852518  0.853635  0.860505   0.846878  0.915526   NaN  \n",
       "4    0.854348  0.855324  0.861271   0.849461  0.917639   NaN  \n",
       "..        ...       ...       ...        ...       ...   ...  \n",
       "106  0.863796  0.864541  0.869531   0.859616  0.925525   NaN  \n",
       "107  0.863029  0.863597  0.867322   0.859918  0.924754   NaN  \n",
       "108  0.863326  0.864018  0.868673   0.859418  0.925477   NaN  \n",
       "109  0.864736  0.865342  0.869477   0.861249  0.926651        \n",
       "110  0.863821  0.864472  0.868847   0.860146  0.926498        \n",
       "\n",
       "[111 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.read_csv('../../output/csv/word2vec.csv')\n",
    "result = pd.concat([result, output_saved_df], axis=0, ignore_index=True)\n",
    "# result.sort_values(by='accuracy', ascending=True)\n",
    "# result[result['feature_extraction']=='Word2Vec(vector_size=100, window=5)']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.to_csv(\"../../output/csv/word2vec.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>length_used</th>\n",
       "      <th>feature_extraction</th>\n",
       "      <th>feature_selection</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=100, window=3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.842649</td>\n",
       "      <td>0.843850</td>\n",
       "      <td>0.850587</td>\n",
       "      <td>0.837234</td>\n",
       "      <td>0.906563</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=100, window=5)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.847769</td>\n",
       "      <td>0.848826</td>\n",
       "      <td>0.855068</td>\n",
       "      <td>0.842681</td>\n",
       "      <td>0.911585</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=100, window=7)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.849649</td>\n",
       "      <td>0.850687</td>\n",
       "      <td>0.856728</td>\n",
       "      <td>0.844732</td>\n",
       "      <td>0.913208</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=200, window=3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.852518</td>\n",
       "      <td>0.853635</td>\n",
       "      <td>0.860505</td>\n",
       "      <td>0.846878</td>\n",
       "      <td>0.915526</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=200, window=5)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.854348</td>\n",
       "      <td>0.855324</td>\n",
       "      <td>0.861271</td>\n",
       "      <td>0.849461</td>\n",
       "      <td>0.917639</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=200, window=7)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.856302</td>\n",
       "      <td>0.857377</td>\n",
       "      <td>0.864039</td>\n",
       "      <td>0.850829</td>\n",
       "      <td>0.918628</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=300, window=3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.856623</td>\n",
       "      <td>0.857629</td>\n",
       "      <td>0.863930</td>\n",
       "      <td>0.851424</td>\n",
       "      <td>0.919816</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=300, window=5)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.859517</td>\n",
       "      <td>0.860394</td>\n",
       "      <td>0.866098</td>\n",
       "      <td>0.854766</td>\n",
       "      <td>0.921931</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=300, window=7)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.859493</td>\n",
       "      <td>0.860598</td>\n",
       "      <td>0.867477</td>\n",
       "      <td>0.853828</td>\n",
       "      <td>0.922632</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=400, window=3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.858973</td>\n",
       "      <td>0.859875</td>\n",
       "      <td>0.865743</td>\n",
       "      <td>0.854087</td>\n",
       "      <td>0.921766</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=400, window=5)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.860704</td>\n",
       "      <td>0.861635</td>\n",
       "      <td>0.867688</td>\n",
       "      <td>0.855668</td>\n",
       "      <td>0.922953</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=400, window=7)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.860506</td>\n",
       "      <td>0.861472</td>\n",
       "      <td>0.867652</td>\n",
       "      <td>0.855393</td>\n",
       "      <td>0.923939</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=500, window=3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.859740</td>\n",
       "      <td>0.860878</td>\n",
       "      <td>0.868183</td>\n",
       "      <td>0.853699</td>\n",
       "      <td>0.922442</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=500, window=5)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.860754</td>\n",
       "      <td>0.861634</td>\n",
       "      <td>0.867355</td>\n",
       "      <td>0.855993</td>\n",
       "      <td>0.923864</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=500, window=7)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.862337</td>\n",
       "      <td>0.863226</td>\n",
       "      <td>0.868957</td>\n",
       "      <td>0.857575</td>\n",
       "      <td>0.924870</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=600, window=3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.859690</td>\n",
       "      <td>0.860789</td>\n",
       "      <td>0.867845</td>\n",
       "      <td>0.853850</td>\n",
       "      <td>0.922766</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=600, window=5)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.860630</td>\n",
       "      <td>0.861637</td>\n",
       "      <td>0.868096</td>\n",
       "      <td>0.855275</td>\n",
       "      <td>0.923922</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=600, window=7)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.862807</td>\n",
       "      <td>0.863743</td>\n",
       "      <td>0.869833</td>\n",
       "      <td>0.857739</td>\n",
       "      <td>0.925050</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=700, window=3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.860432</td>\n",
       "      <td>0.861465</td>\n",
       "      <td>0.868094</td>\n",
       "      <td>0.854939</td>\n",
       "      <td>0.922403</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>osf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Word2Vec(vector_size=700, window=5)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n",
       "      <td>0.860680</td>\n",
       "      <td>0.861683</td>\n",
       "      <td>0.868067</td>\n",
       "      <td>0.855398</td>\n",
       "      <td>0.923877</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data length_used                   feature_extraction  feature_selection  \\\n",
       "0   osf         NaN  Word2Vec(vector_size=100, window=3)                NaN   \n",
       "1   osf         NaN  Word2Vec(vector_size=100, window=5)                NaN   \n",
       "2   osf         NaN  Word2Vec(vector_size=100, window=7)                NaN   \n",
       "3   osf         NaN  Word2Vec(vector_size=200, window=3)                NaN   \n",
       "4   osf         NaN  Word2Vec(vector_size=200, window=5)                NaN   \n",
       "5   osf         NaN  Word2Vec(vector_size=200, window=7)                NaN   \n",
       "6   osf         NaN  Word2Vec(vector_size=300, window=3)                NaN   \n",
       "7   osf         NaN  Word2Vec(vector_size=300, window=5)                NaN   \n",
       "8   osf         NaN  Word2Vec(vector_size=300, window=7)                NaN   \n",
       "9   osf         NaN  Word2Vec(vector_size=400, window=3)                NaN   \n",
       "10  osf         NaN  Word2Vec(vector_size=400, window=5)                NaN   \n",
       "11  osf         NaN  Word2Vec(vector_size=400, window=7)                NaN   \n",
       "12  osf         NaN  Word2Vec(vector_size=500, window=3)                NaN   \n",
       "13  osf         NaN  Word2Vec(vector_size=500, window=5)                NaN   \n",
       "14  osf         NaN  Word2Vec(vector_size=500, window=7)                NaN   \n",
       "15  osf         NaN  Word2Vec(vector_size=600, window=3)                NaN   \n",
       "16  osf         NaN  Word2Vec(vector_size=600, window=5)                NaN   \n",
       "17  osf         NaN  Word2Vec(vector_size=600, window=7)                NaN   \n",
       "18  osf         NaN  Word2Vec(vector_size=700, window=3)                NaN   \n",
       "19  osf         NaN  Word2Vec(vector_size=700, window=5)                NaN   \n",
       "\n",
       "                                                model  accuracy        f1  \\\n",
       "0   LogisticRegression(max_iter=1000, class_weight...  0.842649  0.843850   \n",
       "1   LogisticRegression(max_iter=1000, class_weight...  0.847769  0.848826   \n",
       "2   LogisticRegression(max_iter=1000, class_weight...  0.849649  0.850687   \n",
       "3   LogisticRegression(max_iter=1000, class_weight...  0.852518  0.853635   \n",
       "4   LogisticRegression(max_iter=1000, class_weight...  0.854348  0.855324   \n",
       "5   LogisticRegression(max_iter=1000, class_weight...  0.856302  0.857377   \n",
       "6   LogisticRegression(max_iter=1000, class_weight...  0.856623  0.857629   \n",
       "7   LogisticRegression(max_iter=1000, class_weight...  0.859517  0.860394   \n",
       "8   LogisticRegression(max_iter=1000, class_weight...  0.859493  0.860598   \n",
       "9   LogisticRegression(max_iter=1000, class_weight...  0.858973  0.859875   \n",
       "10  LogisticRegression(max_iter=1000, class_weight...  0.860704  0.861635   \n",
       "11  LogisticRegression(max_iter=1000, class_weight...  0.860506  0.861472   \n",
       "12  LogisticRegression(max_iter=1000, class_weight...  0.859740  0.860878   \n",
       "13  LogisticRegression(max_iter=1000, class_weight...  0.860754  0.861634   \n",
       "14  LogisticRegression(max_iter=1000, class_weight...  0.862337  0.863226   \n",
       "15  LogisticRegression(max_iter=1000, class_weight...  0.859690  0.860789   \n",
       "16  LogisticRegression(max_iter=1000, class_weight...  0.860630  0.861637   \n",
       "17  LogisticRegression(max_iter=1000, class_weight...  0.862807  0.863743   \n",
       "18  LogisticRegression(max_iter=1000, class_weight...  0.860432  0.861465   \n",
       "19  LogisticRegression(max_iter=1000, class_weight...  0.860680  0.861683   \n",
       "\n",
       "      recall  precision   roc_auc  notes  \n",
       "0   0.850587   0.837234  0.906563    NaN  \n",
       "1   0.855068   0.842681  0.911585    NaN  \n",
       "2   0.856728   0.844732  0.913208    NaN  \n",
       "3   0.860505   0.846878  0.915526    NaN  \n",
       "4   0.861271   0.849461  0.917639    NaN  \n",
       "5   0.864039   0.850829  0.918628    NaN  \n",
       "6   0.863930   0.851424  0.919816    NaN  \n",
       "7   0.866098   0.854766  0.921931    NaN  \n",
       "8   0.867477   0.853828  0.922632    NaN  \n",
       "9   0.865743   0.854087  0.921766    NaN  \n",
       "10  0.867688   0.855668  0.922953    NaN  \n",
       "11  0.867652   0.855393  0.923939    NaN  \n",
       "12  0.868183   0.853699  0.922442    NaN  \n",
       "13  0.867355   0.855993  0.923864    NaN  \n",
       "14  0.868957   0.857575  0.924870    NaN  \n",
       "15  0.867845   0.853850  0.922766    NaN  \n",
       "16  0.868096   0.855275  0.923922    NaN  \n",
       "17  0.869833   0.857739  0.925050    NaN  \n",
       "18  0.868094   0.854939  0.922403    NaN  \n",
       "19  0.868067   0.855398  0.923877    NaN  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(r\"../../output/csv/word2vec.csv\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
