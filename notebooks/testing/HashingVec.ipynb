{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7686600,"sourceType":"datasetVersion","datasetId":4485408},{"sourceId":7929032,"sourceType":"datasetVersion","datasetId":4660271},{"sourceId":7957860,"sourceType":"datasetVersion","datasetId":4680948},{"sourceId":7961391,"sourceType":"datasetVersion","datasetId":4683398}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport time\n\n# text NLP\nimport nltk\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport string\nfrom wordcloud import WordCloud\nfrom nltk.corpus import words\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom unidecode import unidecode\n\n# Preprocessing\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\n\n# model\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n# Score\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import KFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-28T08:22:01.186213Z","iopub.execute_input":"2024-03-28T08:22:01.186537Z","iopub.status.idle":"2024-03-28T08:22:05.103804Z","shell.execute_reply.started":"2024-03-28T08:22:01.186510Z","shell.execute_reply":"2024-03-28T08:22:05.102817Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning & Preprocessing","metadata":{}},{"cell_type":"code","source":"# import nltk\n# import subprocess\n\n# # Download and unzip wordnet\n# try:\n#     nltk.data.find('wordnet.zip')\n# except:\n#     nltk.download('wordnet', download_dir='/kaggle/working/')\n#     command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n#     subprocess.run(command.split())\n#     nltk.data.path.append('/kaggle/working/')\n\n# # Now you can import the NLTK resources as usual\n# from nltk.corpus import wordnet","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:43:54.816250Z","iopub.execute_input":"2024-03-24T09:43:54.817111Z","iopub.status.idle":"2024-03-24T09:43:54.821379Z","shell.execute_reply.started":"2024-03-24T09:43:54.817077Z","shell.execute_reply":"2024-03-24T09:43:54.820334Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# class BasicTextCleaning:\n#     def __init__(self):\n#         # define some necessary elements\n#         self.stopwords = set(stopwords.words('english'))\n#         self.words_corpus = set(words.words())\n#         self.stemmer = PorterStemmer()\n#         self.lemmatizer = WordNetLemmatizer()\n\n#         # dictionary of methods can be used\n#         self.methods = {'lowercase': str.lower,\n#                         'accent_removal': self.accent_removal,\n#                         'strip': str.strip,\n#                         'nice_display': self.nice_display,\n#                         'tokenization': nltk.word_tokenize,\n#                         'stemming': self.stemming,\n#                         'lemmatization': self.lemmatization,\n#                         'punctuation_removal': self.punctuation_removal,\n#                         'stopwords_removal': self.stopwords_removal,\n#                         'contractions_expand': self.contractions_expand,\n#                         'nonsense_removal': self.nonsense_removal,\n#                         'number_removal': self.number_removal}\n\n#         self.punctuations = '[%s]' % re.escape(string.punctuation)\n\n#     def text_cleaning(self, texts, methods=None):\n#         if not methods:\n#             methods = ['accent_removal', 'lowercase', 'nice_display', 'punctuation_removal',\n#                        'stopwords_removal', 'lemmatization', 'stemming']\n#         if isinstance(texts, str):\n#             texts = [texts]\n#         cleaned_texts = []\n#         for text in texts:\n#             for method in methods:\n#                 if method not in self.methods.keys():\n#                     raise Warning('Invalid method \"{}\". Basic text cleaning methods available: {}'.format(method, \", \".join(self.methods.keys())))\n#                 text = self.methods[method](text)\n#             cleaned_texts.append(text)\n#         return cleaned_texts\n\n#     def strip_text(self, text):\n#         return text.strip()\n\n#     def lowercase(self, text):\n#         return text.lower()\n\n#     def contractions_expand(self, text):\n#         return contractions.fix(text)\n\n#     def number_removal(self, text):\n#         text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n#         text = re.sub(r'\\s+', ' ', text)\n#         return text\n\n#     def nice_display(self, text):\n#         text = re.sub(r\"([^\\w\\s([{\\'])(\\w)\", r\"\\1 \\2\", text)\n#         text = re.sub(r'\\s+', ' ', text)\n#         return text.strip()\n\n#     def accent_removal(self, text):\n#         text = unidecode(text)\n#         return text\n\n#     def punctuation_removal(self, text):\n#         text = re.sub(self.punctuations, ' ', text)\n#         text = re.sub(r'\\s+', ' ', text)\n#         return text.strip()\n\n#     def stopwords_removal(self, text):\n#         return \" \".join([word for word in text.split() if word not in self.stopwords])\n\n#     def stemming(self, text):\n#         return \" \".join([self.stemmer.stem(word) for word in text.split()])\n\n#     def lemmatization(self, text):\n#         return \" \".join([self.lemmatizer.lemmatize(word) for word in text.split()])\n\n#     def tokenization(self, text):\n#         return nltk.word_tokenize(text)\n\n#     def nonsense_removal(self, text):\n#         return \" \".join([word for word in text.split() if wordnet.synsets(word)])","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:44:09.192104Z","iopub.execute_input":"2024-03-24T09:44:09.192486Z","iopub.status.idle":"2024-03-24T09:44:09.199659Z","shell.execute_reply.started":"2024-03-24T09:44:09.192426Z","shell.execute_reply":"2024-03-24T09:44:09.198722Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# cleaning = BasicTextCleaning()\n# data['text'] = cleaning.text_cleaning(data['text_'])\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:44:20.842649Z","iopub.execute_input":"2024-03-24T09:44:20.843035Z","iopub.status.idle":"2024-03-24T09:44:20.847196Z","shell.execute_reply.started":"2024-03-24T09:44:20.843005Z","shell.execute_reply":"2024-03-24T09:44:20.846278Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/data-cleaning/datacleaning.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:22:51.910256Z","iopub.execute_input":"2024-03-28T08:22:51.911288Z","iopub.status.idle":"2024-03-28T08:22:52.422534Z","shell.execute_reply.started":"2024-03-28T08:22:51.911251Z","shell.execute_reply":"2024-03-28T08:22:52.421474Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data['label'].replace('CG', 1, inplace=True)\ndata['label'].replace('OR', 0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:22:55.061555Z","iopub.execute_input":"2024-03-28T08:22:55.062461Z","iopub.status.idle":"2024-03-28T08:22:55.107383Z","shell.execute_reply.started":"2024-03-28T08:22:55.062427Z","shell.execute_reply":"2024-03-28T08:22:55.106412Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/384321868.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data['label'].replace('CG', 1, inplace=True)\n/tmp/ipykernel_34/384321868.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data['label'].replace('OR', 0, inplace=True)\n/tmp/ipykernel_34/384321868.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  data['label'].replace('OR', 0, inplace=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"data = data.fillna('')","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:23:04.891977Z","iopub.execute_input":"2024-03-28T08:23:04.892292Z","iopub.status.idle":"2024-03-28T08:23:04.917965Z","shell.execute_reply.started":"2024-03-28T08:23:04.892268Z","shell.execute_reply":"2024-03-28T08:23:04.917230Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Save result","metadata":{}},{"cell_type":"code","source":"# data_save = pd.DataFrame(columns=['data','length_used', 'feature_extraction', 'feature_selection', 'model', 'accuracy', 'f1', 'recall', 'precision', 'roc_auc', 'notes'])\n# data_save.to_csv('results.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T14:21:21.632478Z","iopub.execute_input":"2024-03-22T14:21:21.633096Z","iopub.status.idle":"2024-03-22T14:21:21.642507Z","shell.execute_reply.started":"2024-03-22T14:21:21.633063Z","shell.execute_reply":"2024-03-22T14:21:21.641537Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def save_and_print(data, length_used, feature_extraction, feature_selection, model, scores):\n    data_save = pd.read_csv('/kaggle/input/result/results.csv')\n    for feature_extraction in feature_extractions:\n        sc = str(feature_extraction)\n        new_row = {'data': data, 'length_used': length_used, \n               'feature_extraction': feature_extraction, \n               'feature_selection': feature_selection, \n               'model': model, 'accuracy': np.mean(scores[sc]['accuracy']),\n               'f1': np.mean(scores[sc]['f1']), 'recall': np.mean(scores[sc]['recall']), \n               'precision': np.mean(scores[sc]['precision']), 'roc_auc': np.mean(scores[sc]['roc_auc']), \n               'notes': notes}\n        data_save.loc[len(data_save)] = new_row\n        data_save.to_csv('/kaggle/working/results.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:23:11.031361Z","iopub.execute_input":"2024-03-28T08:23:11.032173Z","iopub.status.idle":"2024-03-28T08:23:11.039338Z","shell.execute_reply.started":"2024-03-28T08:23:11.032141Z","shell.execute_reply":"2024-03-28T08:23:11.038390Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X, y = data['text'], data['label']","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:23:17.689049Z","iopub.execute_input":"2024-03-28T08:23:17.689638Z","iopub.status.idle":"2024-03-28T08:23:17.695944Z","shell.execute_reply.started":"2024-03-28T08:23:17.689605Z","shell.execute_reply":"2024-03-28T08:23:17.694862Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Case 1\nDon't use feature lengh, feature selection","metadata":{}},{"cell_type":"code","source":"dataname = 'Fake Review Dataset'\nlengh_used = None\nfeature_selection = None\nnotes = None","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:23:19.869654Z","iopub.execute_input":"2024-03-28T08:23:19.870301Z","iopub.status.idle":"2024-03-28T08:23:19.874552Z","shell.execute_reply.started":"2024-03-28T08:23:19.870271Z","shell.execute_reply":"2024-03-28T08:23:19.873585Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Ngram = (1,1)","metadata":{}},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:54:02.328490Z","iopub.execute_input":"2024-03-27T22:54:02.328848Z","iopub.status.idle":"2024-03-27T22:54:06.695242Z","shell.execute_reply.started":"2024-03-27T22:54:02.328821Z","shell.execute_reply":"2024-03-27T22:54:06.694436Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n\nfolds = kf.split(X, y)\nscores = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_lg = lgb.LGBMClassifier(force_row_wise=True)\n        model_lg.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_lg.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_lg.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores:\n            scores[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores[key]['accuracy'].append(accuracy)\n        scores[key]['f1'].append(f1)\n        scores[key]['recall'].append(recall)\n        scores[key]['precision'].append(precision)\n        scores[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T11:09:23.918631Z","iopub.execute_input":"2024-03-24T11:09:23.919366Z","iopub.status.idle":"2024-03-24T11:11:10.022005Z","shell.execute_reply.started":"2024-03-24T11:09:23.919328Z","shell.execute_reply":"2024-03-24T11:11:10.021053Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 25499\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 158972\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 3270\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 25498\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 159236\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 3276\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 25498\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 157115\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 3251\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 12749\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 25499\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 158775\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 3276\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 25497\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 158805\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 3296\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n","output_type":"stream"}]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lg, scores)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T11:33:36.436013Z","iopub.execute_input":"2024-03-24T11:33:36.436394Z","iopub.status.idle":"2024-03-24T11:33:36.467783Z","shell.execute_reply.started":"2024-03-24T11:33:36.436362Z","shell.execute_reply":"2024-03-24T11:33:36.466902Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"# XG boost","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:54:11.874686Z","iopub.execute_input":"2024-03-27T22:54:11.875576Z","iopub.status.idle":"2024-03-27T22:54:11.879350Z","shell.execute_reply.started":"2024-03-27T22:54:11.875544Z","shell.execute_reply":"2024-03-27T22:54:11.878441Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n\nfolds = kf.split(X, y)\nscores_xgb = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_xgb = XGBClassifier()\n        model_xgb.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_xgb.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_xgb.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_xgb:\n            scores_xgb[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_xgb[key]['accuracy'].append(accuracy)\n        scores_xgb[key]['f1'].append(f1)\n        scores_xgb[key]['recall'].append(recall)\n        scores_xgb[key]['precision'].append(precision)\n        scores_xgb[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T11:14:48.664840Z","iopub.execute_input":"2024-03-24T11:14:48.665743Z","iopub.status.idle":"2024-03-24T11:24:53.058440Z","shell.execute_reply.started":"2024-03-24T11:14:48.665707Z","shell.execute_reply":"2024-03-24T11:24:53.057464Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_xgb, scores_xgb)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T11:34:28.498893Z","iopub.execute_input":"2024-03-24T11:34:28.499635Z","iopub.status.idle":"2024-03-24T11:34:28.539799Z","shell.execute_reply.started":"2024-03-24T11:34:28.499598Z","shell.execute_reply":"2024-03-24T11:34:28.538706Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"# Adaboost","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:58:05.865792Z","iopub.execute_input":"2024-03-28T02:58:05.866131Z","iopub.status.idle":"2024-03-28T02:58:05.870366Z","shell.execute_reply.started":"2024-03-28T02:58:05.866106Z","shell.execute_reply":"2024-03-28T02:58:05.869417Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n\nfolds = kf.split(X, y)\nscores_ada = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_ada = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)\n        model_ada.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_ada.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_ada.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_ada:\n            scores_ada[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_ada[key]['accuracy'].append(accuracy)\n        scores_ada[key]['f1'].append(f1)\n        scores_ada[key]['recall'].append(recall)\n        scores_ada[key]['precision'].append(precision)\n        scores_ada[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:34:37.946831Z","iopub.execute_input":"2024-03-24T15:34:37.947748Z","iopub.status.idle":"2024-03-24T15:44:48.086181Z","shell.execute_reply.started":"2024-03-24T15:34:37.947713Z","shell.execute_reply":"2024-03-24T15:44:48.084958Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_ada, scores_ada)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T16:17:28.642738Z","iopub.execute_input":"2024-03-24T16:17:28.643415Z","iopub.status.idle":"2024-03-24T16:17:28.672748Z","shell.execute_reply.started":"2024-03-24T16:17:28.643384Z","shell.execute_reply":"2024-03-24T16:17:28.672022Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# CATBOOST","metadata":{}},{"cell_type":"code","source":"import catboost as cb","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:23:28.988854Z","iopub.execute_input":"2024-03-28T08:23:28.989646Z","iopub.status.idle":"2024-03-28T08:23:29.191459Z","shell.execute_reply.started":"2024-03-28T08:23:28.989602Z","shell.execute_reply":"2024-03-28T08:23:29.190055Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"folds = kf.split(X, y)\nscores_cat = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_cat = cb.CatBoostClassifier(\n            random_state=42,\n            task_type='GPU',\n            border_count=2 * len(feature_extractions)\n        )\n\n        model_cat.fit(\n            X_train_hashed,\n            y_train,\n            verbose=False,\n            plot=False,\n            early_stopping_rounds=50,\n            use_best_model=True,\n            eval_set=(X_test_hashed, y_test)\n        )\n\n        y_test_pred = model_cat.predict(X_test_hashed)\n\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_cat.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_cat:\n            scores_cat[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_cat[key]['accuracy'].append(accuracy)\n        scores_cat[key]['f1'].append(f1)\n        scores_cat[key]['recall'].append(recall)\n        scores_cat[key]['precision'].append(precision)\n        scores_cat[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:54:59.959174Z","iopub.execute_input":"2024-03-24T15:54:59.959777Z","iopub.status.idle":"2024-03-24T16:14:34.833742Z","shell.execute_reply.started":"2024-03-24T15:54:59.959746Z","shell.execute_reply":"2024-03-24T16:14:34.832801Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_cat, scores_cat)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T16:16:38.146955Z","iopub.execute_input":"2024-03-24T16:16:38.147849Z","iopub.status.idle":"2024-03-24T16:16:38.176076Z","shell.execute_reply.started":"2024-03-24T16:16:38.147814Z","shell.execute_reply":"2024-03-24T16:16:38.175073Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n\nfolds = kf.split(X, y)\nscores_lr = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_lr = LogisticRegression(max_iter = 1000)\n        model_lr.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_lr.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_lr.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_lr:\n            scores_lr[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_lr[key]['accuracy'].append(accuracy)\n        scores_lr[key]['f1'].append(f1)\n        scores_lr[key]['recall'].append(recall)\n        scores_lr[key]['precision'].append(precision)\n        scores_lr[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T12:10:22.334468Z","iopub.execute_input":"2024-03-15T12:10:22.334843Z","iopub.status.idle":"2024-03-15T12:13:07.614695Z","shell.execute_reply.started":"2024-03-15T12:10:22.334814Z","shell.execute_reply":"2024-03-15T12:13:07.613872Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lr, scores_lr)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T12:14:08.887605Z","iopub.execute_input":"2024-03-15T12:14:08.887924Z","iopub.status.idle":"2024-03-15T12:14:08.916658Z","shell.execute_reply.started":"2024-03-15T12:14:08.887900Z","shell.execute_reply":"2024-03-15T12:14:08.915827Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n                     HashingVectorizer(n_features = 100),HashingVectorizer()]\nmetrics_param = ['cosine','minkowski']\nneighbors_param = [5, 10, 50, 100]\nfolds = kf.split(X, y)\nscores_knn = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n        for metric in metrics_param:\n            for neighbor in neighbors_param:\n                model_knn = KNeighborsClassifier(metric = metric, n_neighbors = neighbor)\n                model_knn.fit(X_train_hashed, y_train)\n\n                y_test_pred = model_knn.predict(X_test_hashed)\n                accuracy = accuracy_score(y_test, y_test_pred)\n                f1 = f1_score(y_test, y_test_pred, pos_label=1)\n                recall = recall_score(y_test, y_test_pred)\n                precision = precision_score(y_test, y_test_pred)\n                roc_auc = roc_auc_score(y_test, model_knn.predict_proba(X_test_hashed)[:, 1])\n\n                key = str(feature_extraction)\n                if key not in scores_knn:\n                    scores_knn[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n                scores_knn[key]['accuracy'].append(accuracy)\n                scores_knn[key]['f1'].append(f1)\n                scores_knn[key]['recall'].append(recall)\n                scores_knn[key]['precision'].append(precision)\n                scores_knn[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T12:14:33.554596Z","iopub.execute_input":"2024-03-15T12:14:33.554948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_knn, scores_knn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ngram = (1,2)","metadata":{}},{"cell_type":"markdown","source":"# Logistic ","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 2)), \n                       HashingVectorizer(n_features = 50, ngram_range=(1, 2)),\n                     HashingVectorizer(n_features = 100, ngram_range=(1, 2)),\n                       HashingVectorizer(ngram_range=(1, 2))]\n\nfolds = kf.split(X, y)\nscores_lr = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_lr = LogisticRegression(max_iter = 1000)\n        model_lr.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_lr.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_lr.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_lr:\n            scores_lr[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_lr[key]['accuracy'].append(accuracy)\n        scores_lr[key]['f1'].append(f1)\n        scores_lr[key]['recall'].append(recall)\n        scores_lr[key]['precision'].append(precision)\n        scores_lr[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T15:02:25.521468Z","iopub.execute_input":"2024-03-27T15:02:25.522131Z","iopub.status.idle":"2024-03-27T15:05:35.869125Z","shell.execute_reply.started":"2024-03-27T15:02:25.522099Z","shell.execute_reply":"2024-03-27T15:05:35.868275Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lr, scores_lr)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T15:05:58.509988Z","iopub.execute_input":"2024-03-27T15:05:58.510712Z","iopub.status.idle":"2024-03-27T15:05:58.565915Z","shell.execute_reply.started":"2024-03-27T15:05:58.510678Z","shell.execute_reply":"2024-03-27T15:05:58.565175Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 2)), \n                       HashingVectorizer(n_features = 50, ngram_range=(1, 2)),\n                     HashingVectorizer(n_features = 100, ngram_range=(1, 2)),\n                       HashingVectorizer(ngram_range=(1, 2))]\n\nfolds = kf.split(X, y)\nscores = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_lg = lgb.LGBMClassifier(force_row_wise=True)\n        model_lg.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_lg.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_lg.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores:\n            scores[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores[key]['accuracy'].append(accuracy)\n        scores[key]['f1'].append(f1)\n        scores[key]['recall'].append(recall)\n        scores[key]['precision'].append(precision)\n        scores[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T15:10:52.780632Z","iopub.execute_input":"2024-03-27T15:10:52.781001Z","iopub.status.idle":"2024-03-27T15:13:14.026294Z","shell.execute_reply.started":"2024-03-27T15:10:52.780974Z","shell.execute_reply":"2024-03-27T15:13:14.025116Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 233865\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 7457\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 234705\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 7466\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 232285\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 7412\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 234705\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 7485\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 233459\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 7438\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n","output_type":"stream"}]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lg, scores)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T15:13:46.844075Z","iopub.execute_input":"2024-03-27T15:13:46.844891Z","iopub.status.idle":"2024-03-27T15:13:46.877790Z","shell.execute_reply.started":"2024-03-27T15:13:46.844857Z","shell.execute_reply":"2024-03-27T15:13:46.877077Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# XGboost","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 2)), \n                       HashingVectorizer(n_features = 50, ngram_range=(1, 2)),\n                     HashingVectorizer(n_features = 100, ngram_range=(1, 2)),\n                       HashingVectorizer(ngram_range=(1, 2))]\n\nfolds = kf.split(X, y)\nscores_xgb = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_xgb = XGBClassifier()\n        model_xgb.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_xgb.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_xgb.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_xgb:\n            scores_xgb[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_xgb[key]['accuracy'].append(accuracy)\n        scores_xgb[key]['f1'].append(f1)\n        scores_xgb[key]['recall'].append(recall)\n        scores_xgb[key]['precision'].append(precision)\n        scores_xgb[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T15:14:15.251179Z","iopub.execute_input":"2024-03-27T15:14:15.251999Z","iopub.status.idle":"2024-03-27T15:24:51.583910Z","shell.execute_reply.started":"2024-03-27T15:14:15.251969Z","shell.execute_reply":"2024-03-27T15:24:51.583082Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_xgb, scores_xgb)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T15:25:30.541664Z","iopub.execute_input":"2024-03-27T15:25:30.542348Z","iopub.status.idle":"2024-03-27T15:25:30.581330Z","shell.execute_reply.started":"2024-03-27T15:25:30.542316Z","shell.execute_reply":"2024-03-27T15:25:30.580289Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# ADA Boost","metadata":{}},{"cell_type":"code","source":"# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# feature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 2)), \n#                        HashingVectorizer(n_features = 50, ngram_range=(1, 2)),\n#                      HashingVectorizer(n_features = 100, ngram_range=(1, 2)),\n#                        HashingVectorizer(ngram_range=(1, 2))]\n\nfolds = kf.split(X, y)\nscores_ada = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_ada = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)\n        model_ada.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_ada.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_ada.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_ada:\n            scores_ada[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_ada[key]['accuracy'].append(accuracy)\n        scores_ada[key]['f1'].append(f1)\n        scores_ada[key]['recall'].append(recall)\n        scores_ada[key]['precision'].append(precision)\n        scores_ada[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T15:26:17.717114Z","iopub.execute_input":"2024-03-27T15:26:17.717489Z","iopub.status.idle":"2024-03-27T17:12:11.435728Z","shell.execute_reply.started":"2024-03-27T15:26:17.717459Z","shell.execute_reply":"2024-03-27T17:12:11.434913Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_ada, scores_ada)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T17:12:11.995552Z","iopub.execute_input":"2024-03-27T17:12:11.995899Z","iopub.status.idle":"2024-03-27T17:12:12.027455Z","shell.execute_reply.started":"2024-03-27T17:12:11.995872Z","shell.execute_reply":"2024-03-27T17:12:12.026496Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# CAT boost","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 2)), \n                       HashingVectorizer(n_features = 50, ngram_range=(1, 2)),\n                     HashingVectorizer(n_features = 100, ngram_range=(1, 2)),\n                       HashingVectorizer(ngram_range=(1, 2))]\n\nfolds = kf.split(X, y)\nscores_cat = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_cat = cb.CatBoostClassifier(\n            random_state=42,\n            task_type='GPU',\n            border_count=2 * len(feature_extractions)\n        )\n\n        model_cat.fit(\n            X_train_hashed,\n            y_train,\n            verbose=False,\n            plot=False,\n            early_stopping_rounds=50,\n            use_best_model=True,\n            eval_set=(X_test_hashed, y_test)\n        )\n\n        y_test_pred = model_cat.predict(X_test_hashed)\n\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_cat.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_cat:\n            scores_cat[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_cat[key]['accuracy'].append(accuracy)\n        scores_cat[key]['f1'].append(f1)\n        scores_cat[key]['recall'].append(recall)\n        scores_cat[key]['precision'].append(precision)\n        scores_cat[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:54:59.412722Z","iopub.execute_input":"2024-03-27T22:54:59.413103Z","iopub.status.idle":"2024-03-28T00:18:07.200952Z","shell.execute_reply.started":"2024-03-27T22:54:59.413072Z","shell.execute_reply":"2024-03-28T00:18:07.200123Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_cat, scores_cat)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T00:18:41.691005Z","iopub.execute_input":"2024-03-28T00:18:41.691373Z","iopub.status.idle":"2024-03-28T00:18:41.748572Z","shell.execute_reply.started":"2024-03-28T00:18:41.691345Z","shell.execute_reply":"2024-03-28T00:18:41.747563Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Ngram = (1,3)","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 3)), \n                       HashingVectorizer(n_features = 50, ngram_range=(1, 3)),\n                     HashingVectorizer(n_features = 100, ngram_range=(1, 3)),\n                       HashingVectorizer(ngram_range=(1, 3))]","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:24:27.277322Z","iopub.execute_input":"2024-03-28T08:24:27.277695Z","iopub.status.idle":"2024-03-28T08:24:27.283725Z","shell.execute_reply.started":"2024-03-28T08:24:27.277663Z","shell.execute_reply":"2024-03-28T08:24:27.282496Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Logistic","metadata":{}},{"cell_type":"code","source":"folds = kf.split(X, y)\nscores_lr = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_lr = LogisticRegression(max_iter = 1000)\n        model_lr.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_lr.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_lr.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_lr:\n            scores_lr[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_lr[key]['accuracy'].append(accuracy)\n        scores_lr[key]['f1'].append(f1)\n        scores_lr[key]['recall'].append(recall)\n        scores_lr[key]['precision'].append(precision)\n        scores_lr[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T00:19:38.071913Z","iopub.execute_input":"2024-03-28T00:19:38.072272Z","iopub.status.idle":"2024-03-28T00:23:06.865474Z","shell.execute_reply.started":"2024-03-28T00:19:38.072243Z","shell.execute_reply":"2024-03-28T00:23:06.863961Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lr, scores_lr)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T00:24:41.026131Z","iopub.execute_input":"2024-03-28T00:24:41.026957Z","iopub.status.idle":"2024-03-28T00:24:41.060699Z","shell.execute_reply.started":"2024-03-28T00:24:41.026925Z","shell.execute_reply":"2024-03-28T00:24:41.059839Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"folds = kf.split(X, y)\nscores = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_lg = lgb.LGBMClassifier(force_row_wise=True)\n        model_lg.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_lg.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_lg.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores:\n            scores[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores[key]['accuracy'].append(accuracy)\n        scores[key]['f1'].append(f1)\n        scores[key]['recall'].append(recall)\n        scores[key]['precision'].append(precision)\n        scores[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T00:25:02.807407Z","iopub.execute_input":"2024-03-28T00:25:02.807749Z","iopub.status.idle":"2024-03-28T00:27:59.076020Z","shell.execute_reply.started":"2024-03-28T00:25:02.807723Z","shell.execute_reply":"2024-03-28T00:27:59.074948Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 274844\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 8934\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 276631\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 9020\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 273752\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 8901\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 275954\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 9012\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 274746\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 8942\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n","output_type":"stream"}]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lg, scores)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T00:31:24.843273Z","iopub.execute_input":"2024-03-28T00:31:24.844046Z","iopub.status.idle":"2024-03-28T00:31:24.881137Z","shell.execute_reply.started":"2024-03-28T00:31:24.844014Z","shell.execute_reply":"2024-03-28T00:31:24.880334Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# XG Boost","metadata":{}},{"cell_type":"code","source":"folds = kf.split(X, y)\nscores_xgb = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_xgb = XGBClassifier()\n        model_xgb.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_xgb.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_xgb.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_xgb:\n            scores_xgb[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_xgb[key]['accuracy'].append(accuracy)\n        scores_xgb[key]['f1'].append(f1)\n        scores_xgb[key]['recall'].append(recall)\n        scores_xgb[key]['precision'].append(precision)\n        scores_xgb[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T00:31:44.656830Z","iopub.execute_input":"2024-03-28T00:31:44.657812Z","iopub.status.idle":"2024-03-28T00:44:10.297036Z","shell.execute_reply.started":"2024-03-28T00:31:44.657778Z","shell.execute_reply":"2024-03-28T00:44:10.296094Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_xgb, scores_xgb)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T00:45:30.829263Z","iopub.execute_input":"2024-03-28T00:45:30.830059Z","iopub.status.idle":"2024-03-28T00:45:30.876509Z","shell.execute_reply.started":"2024-03-28T00:45:30.830028Z","shell.execute_reply":"2024-03-28T00:45:30.875455Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# ADA Boost","metadata":{}},{"cell_type":"code","source":"folds = kf.split(X, y)\nscores_ada = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_ada = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)\n        model_ada.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_ada.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_ada.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_ada:\n            scores_ada[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_ada[key]['accuracy'].append(accuracy)\n        scores_ada[key]['f1'].append(f1)\n        scores_ada[key]['recall'].append(recall)\n        scores_ada[key]['precision'].append(precision)\n        scores_ada[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T03:00:18.029746Z","iopub.execute_input":"2024-03-28T03:00:18.030466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_ada, scores_ada)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CAT Boost","metadata":{}},{"cell_type":"code","source":"folds = kf.split(X, y)\nscores_cat = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_cat = cb.CatBoostClassifier(\n            random_state=42,\n            task_type='GPU',\n            border_count=2 * len(feature_extractions)\n        )\n\n        model_cat.fit(\n            X_train_hashed,\n            y_train,\n            verbose=False,\n            plot=False,\n            early_stopping_rounds=50,\n            use_best_model=True,\n            eval_set=(X_test_hashed, y_test)\n        )\n\n        y_test_pred = model_cat.predict(X_test_hashed)\n\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_cat.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_cat:\n            scores_cat[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_cat[key]['accuracy'].append(accuracy)\n        scores_cat[key]['f1'].append(f1)\n        scores_cat[key]['recall'].append(recall)\n        scores_cat[key]['precision'].append(precision)\n        scores_cat[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:24:38.987635Z","iopub.execute_input":"2024-03-28T08:24:38.988338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_cat, scores_cat)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result","metadata":{}},{"cell_type":"code","source":"result = pd.read_csv('/kaggle/working/results.csv')\n# result = result.sort_values(by='accuracy', ascending=False)\nresult","metadata":{"execution":{"iopub.status.busy":"2024-03-28T01:44:50.296205Z","iopub.execute_input":"2024-03-28T01:44:50.296606Z","iopub.status.idle":"2024-03-28T01:44:50.318173Z","shell.execute_reply.started":"2024-03-28T01:44:50.296577Z","shell.execute_reply":"2024-03-28T01:44:50.317239Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"                   data  length_used  \\\n0   Fake Review Dataset          NaN   \n1   Fake Review Dataset          NaN   \n2   Fake Review Dataset          NaN   \n3   Fake Review Dataset          NaN   \n4   Fake Review Dataset          NaN   \n..                  ...          ...   \n74  Fake Review Dataset          NaN   \n75  Fake Review Dataset          NaN   \n76  Fake Review Dataset          NaN   \n77  Fake Review Dataset          NaN   \n78  Fake Review Dataset          NaN   \n\n                                   feature_extraction  feature_selection  \\\n0                    HashingVectorizer(n_features=10)                NaN   \n1                    HashingVectorizer(n_features=50)                NaN   \n2                   HashingVectorizer(n_features=100)                NaN   \n3                                 HashingVectorizer()                NaN   \n4                    HashingVectorizer(n_features=10)                NaN   \n..                                                ...                ...   \n74              HashingVectorizer(ngram_range=(1, 3))                NaN   \n75  HashingVectorizer(n_features=10, ngram_range=(...                NaN   \n76  HashingVectorizer(n_features=50, ngram_range=(...                NaN   \n77  HashingVectorizer(n_features=100, ngram_range=...                NaN   \n78              HashingVectorizer(ngram_range=(1, 3))                NaN   \n\n                                                model  accuracy        f1  \\\n0                   LogisticRegression(max_iter=1000)  0.567818  0.580897   \n1                   LogisticRegression(max_iter=1000)  0.631950  0.635951   \n2                   LogisticRegression(max_iter=1000)  0.676049  0.678463   \n3                   LogisticRegression(max_iter=1000)  0.854472  0.854731   \n4   KNeighborsClassifier(metric='euclidean', n_nei...  0.560846  0.568853   \n..                                                ...       ...       ...   \n74  XGBClassifier(base_score=None, booster=None, c...  0.852221  0.847779   \n75               AdaBoostClassifier(n_estimators=100)  0.560158  0.561191   \n76               AdaBoostClassifier(n_estimators=100)  0.648386  0.647011   \n77               AdaBoostClassifier(n_estimators=100)  0.684865  0.686397   \n78               AdaBoostClassifier(n_estimators=100)  0.797329  0.792084   \n\n      recall  precision   roc_auc  notes  \n0   0.599094   0.563848  0.596594    NaN  \n1   0.643011   0.629166  0.681856    NaN  \n2   0.683620   0.673423  0.738315    NaN  \n3   0.856315   0.853172  0.932805    NaN  \n4   0.584163   0.555935  0.579758    NaN  \n..       ...        ...       ...    ...  \n74  0.823152   0.873932  0.936123    NaN  \n75  0.558183   0.564506  0.583923    NaN  \n76  0.639542   0.654850  0.703554    NaN  \n77  0.684461   0.688559  0.748474    NaN  \n78  0.777390   0.807344  0.889228    NaN  \n\n[79 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>data</th>\n      <th>length_used</th>\n      <th>feature_extraction</th>\n      <th>feature_selection</th>\n      <th>model</th>\n      <th>accuracy</th>\n      <th>f1</th>\n      <th>recall</th>\n      <th>precision</th>\n      <th>roc_auc</th>\n      <th>notes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=10)</td>\n      <td>NaN</td>\n      <td>LogisticRegression(max_iter=1000)</td>\n      <td>0.567818</td>\n      <td>0.580897</td>\n      <td>0.599094</td>\n      <td>0.563848</td>\n      <td>0.596594</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=50)</td>\n      <td>NaN</td>\n      <td>LogisticRegression(max_iter=1000)</td>\n      <td>0.631950</td>\n      <td>0.635951</td>\n      <td>0.643011</td>\n      <td>0.629166</td>\n      <td>0.681856</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=100)</td>\n      <td>NaN</td>\n      <td>LogisticRegression(max_iter=1000)</td>\n      <td>0.676049</td>\n      <td>0.678463</td>\n      <td>0.683620</td>\n      <td>0.673423</td>\n      <td>0.738315</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer()</td>\n      <td>NaN</td>\n      <td>LogisticRegression(max_iter=1000)</td>\n      <td>0.854472</td>\n      <td>0.854731</td>\n      <td>0.856315</td>\n      <td>0.853172</td>\n      <td>0.932805</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=10)</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier(metric='euclidean', n_nei...</td>\n      <td>0.560846</td>\n      <td>0.568853</td>\n      <td>0.584163</td>\n      <td>0.555935</td>\n      <td>0.579758</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(ngram_range=(1, 3))</td>\n      <td>NaN</td>\n      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n      <td>0.852221</td>\n      <td>0.847779</td>\n      <td>0.823152</td>\n      <td>0.873932</td>\n      <td>0.936123</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=10, ngram_range=(...</td>\n      <td>NaN</td>\n      <td>AdaBoostClassifier(n_estimators=100)</td>\n      <td>0.560158</td>\n      <td>0.561191</td>\n      <td>0.558183</td>\n      <td>0.564506</td>\n      <td>0.583923</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=50, ngram_range=(...</td>\n      <td>NaN</td>\n      <td>AdaBoostClassifier(n_estimators=100)</td>\n      <td>0.648386</td>\n      <td>0.647011</td>\n      <td>0.639542</td>\n      <td>0.654850</td>\n      <td>0.703554</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=100, ngram_range=...</td>\n      <td>NaN</td>\n      <td>AdaBoostClassifier(n_estimators=100)</td>\n      <td>0.684865</td>\n      <td>0.686397</td>\n      <td>0.684461</td>\n      <td>0.688559</td>\n      <td>0.748474</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(ngram_range=(1, 3))</td>\n      <td>NaN</td>\n      <td>AdaBoostClassifier(n_estimators=100)</td>\n      <td>0.797329</td>\n      <td>0.792084</td>\n      <td>0.777390</td>\n      <td>0.807344</td>\n      <td>0.889228</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>79 rows × 11 columns</p>\n</div>"},"metadata":{}}]}]}